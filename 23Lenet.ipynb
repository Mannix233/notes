{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a4413d-744b-4840-9a96-3c4007e26335",
   "metadata": {},
   "source": [
    "# LeNet：详细讲解\n",
    "\n",
    "### 核心思想！\n",
    "\n",
    "```\n",
    "图片 → 卷积提取特征 → 池化降低敏感性 → 全连接分类\n",
    "\n",
    "这个思路到现在都没变！\n",
    "所有CNN都是这个套路\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、LeNet的结构（逐层理解）\n",
    "\n",
    "### 整体流程\n",
    "\n",
    "```\n",
    "输入(1×28×28)\n",
    "    ↓\n",
    "卷积层1 → 激活 → 池化层1     ← 第1组：提取简单特征\n",
    "    ↓\n",
    "卷积层2 → 激活 → 池化层2     ← 第2组：提取复杂特征\n",
    "    ↓\n",
    "拉平(Flatten)                 ← 把图片变成一维向量\n",
    "    ↓\n",
    "全连接层1 → 激活              ← MLP做分类\n",
    "全连接层2 → 激活\n",
    "全连接层3(输出层)              ← 输出10个类别\n",
    "```\n",
    "\n",
    "### 数据怎么变的（最重要！）\n",
    "\n",
    "```\n",
    "输入：        1 × 28 × 28    ← 1通道，28×28的灰度图\n",
    "\n",
    "Conv1:        6 × 28 × 28    ← 通道1→6，大小不变(padding=2)\n",
    "Sigmoid:      6 × 28 × 28    ← 不变\n",
    "AvgPool:      6 × 14 × 14    ← 通道不变，大小减半\n",
    "\n",
    "Conv2:       16 × 10 × 10    ← 通道6→16，大小变小(没padding)\n",
    "Sigmoid:     16 × 10 × 10    ← 不变\n",
    "AvgPool:     16 × 5  × 5     ← 通道不变，大小减半\n",
    "\n",
    "Flatten:     400              ← 16×5×5=400，拉成一维\n",
    "\n",
    "Linear1:     120              ← 400→120\n",
    "Linear2:     84               ← 120→84\n",
    "Linear3:     10               ← 84→10（10个数字类别）\n",
    "```\n",
    "\n",
    "**规律：**\n",
    "\n",
    "```\n",
    "卷积部分：通道越来越多（1→6→16），空间越来越小（28→14→10→5）\n",
    "全连接部分：逐渐压缩到类别数（400→120→84→10）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、代码实现\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # ===== 第1组：卷积 + 激活 + 池化 =====\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),   # 1\n",
    "    nn.Sigmoid(),                                  # 2\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),         # 3\n",
    "    \n",
    "    # ===== 第2组：卷积 + 激活 + 池化 =====\n",
    "    nn.Conv2d(6, 16, kernel_size=5),               # 4\n",
    "    nn.Sigmoid(),                                   # 5\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),          # 6\n",
    "    \n",
    "    # ===== 拉平 =====\n",
    "    nn.Flatten(),                                   # 7\n",
    "    \n",
    "    # ===== 全连接(MLP) =====\n",
    "    nn.Linear(16 * 5 * 5, 120),                    # 8\n",
    "    nn.Sigmoid(),                                   # 9\n",
    "    nn.Linear(120, 84),                            # 10\n",
    "    nn.Sigmoid(),                                   # 11\n",
    "    nn.Linear(84, 10),                             # 12\n",
    ")\n",
    "```\n",
    "\n",
    "### 逐行翻译\n",
    "\n",
    "**第1行：第一个卷积层**\n",
    "\n",
    "```python\n",
    "nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
    "```\n",
    "\n",
    "```\n",
    "输入通道=1：灰度图（黑白）\n",
    "输出通道=6：提取6种特征\n",
    "核大小=5：5×5的窗口\n",
    "padding=2：填充2圈，让输出大小不变\n",
    "\n",
    "为什么padding=2？\n",
    "    原始LeNet输入是32×32\n",
    "    我们用的数据是28×28\n",
    "    加padding=2让效果等价于32×32输入\n",
    "    这样输出还是28×28（大小不变）\n",
    "```\n",
    "\n",
    "**第2行：激活函数**\n",
    "\n",
    "```python\n",
    "nn.Sigmoid()\n",
    "```\n",
    "\n",
    "```\n",
    "LeNet是1989年的网络，当时还没有ReLU\n",
    "现在大家都用ReLU，但这里为了还原原始设计用Sigmoid\n",
    "```\n",
    "\n",
    "**第3行：池化层**\n",
    "\n",
    "```python\n",
    "nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "```\n",
    "\n",
    "```\n",
    "2×2的平均池化\n",
    "stride=2：窗口不重叠\n",
    "效果：28×28 → 14×14（大小减半）\n",
    "通道数不变：还是6\n",
    "```\n",
    "\n",
    "**第4行：第二个卷积层**\n",
    "\n",
    "```python\n",
    "nn.Conv2d(6, 16, kernel_size=5)\n",
    "```\n",
    "\n",
    "```\n",
    "输入通道=6：接收上一层的6个通道\n",
    "输出通道=16：提取16种更复杂的特征\n",
    "核大小=5\n",
    "没有padding！所以输出会变小：14-5+1=10\n",
    "```\n",
    "\n",
    "**第5-6行：和前面一样**\n",
    "\n",
    "```\n",
    "Sigmoid激活 → 池化减半\n",
    "10×10 → 5×5\n",
    "```\n",
    "\n",
    "**第7行：拉平**\n",
    "\n",
    "```python\n",
    "nn.Flatten()\n",
    "```\n",
    "\n",
    "```\n",
    "把4维的(批量, 16, 5, 5)变成2维的(批量, 400)\n",
    "16 × 5 × 5 = 400\n",
    "因为全连接层只能接收一维向量\n",
    "```\n",
    "\n",
    "**第8行：第一个全连接层**\n",
    "\n",
    "```python\n",
    "nn.Linear(16 * 5 * 5, 120)\n",
    "```\n",
    "\n",
    "```\n",
    "输入：16×5×5 = 400\n",
    "输出：120\n",
    "\n",
    "为什么输入是16*5*5？\n",
    "    因为拉平前最后一层的输出是 16通道 × 5高 × 5宽\n",
    "    拉平后就是 16×5×5 = 400\n",
    "```\n",
    "\n",
    "**第10-12行：继续压缩**\n",
    "\n",
    "```python\n",
    "nn.Linear(120, 84)     # 120 → 84\n",
    "nn.Linear(84, 10)      # 84 → 10（最终输出10个类别）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、怎么检查每层的输出形状\n",
    "\n",
    "```python\n",
    "X = torch.rand(size=(1, 1, 28, 28))    # 1张图，1通道，28×28\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, '\\t output shape:', X.shape)\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "Conv2d           output shape: torch.Size([1, 6, 28, 28])\n",
    "Sigmoid          output shape: torch.Size([1, 6, 28, 28])\n",
    "AvgPool2d        output shape: torch.Size([1, 6, 14, 14])\n",
    "Conv2d           output shape: torch.Size([1, 16, 10, 10])\n",
    "Sigmoid          output shape: torch.Size([1, 16, 10, 10])\n",
    "AvgPool2d        output shape: torch.Size([1, 16, 5, 5])\n",
    "Flatten          output shape: torch.Size([1, 400])\n",
    "Linear           output shape: torch.Size([1, 120])\n",
    "Sigmoid          output shape: torch.Size([1, 120])\n",
    "Linear           output shape: torch.Size([1, 84])\n",
    "Sigmoid          output shape: torch.Size([1, 84])\n",
    "Linear           output shape: torch.Size([1, 10])\n",
    "```\n",
    "\n",
    "**这个技巧很实用！不确定输出形状时，跑一下就知道了。**\n",
    "\n",
    "---\n",
    "\n",
    "## 五、训练部分\n",
    "\n",
    "### 你需要知道的核心改动\n",
    "\n",
    "**和之前MLP训练的唯一区别：数据和模型要搬到GPU上**\n",
    "\n",
    "```python\n",
    "# 选设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 模型搬到GPU\n",
    "net.to(device)\n",
    "\n",
    "# 训练循环中，每个batch的数据也要搬到GPU\n",
    "for X, y in train_loader:\n",
    "    X = X.to(device)        # 数据搬到GPU\n",
    "    y = y.to(device)        # 标签搬到GPU\n",
    "    y_hat = net(X)           # 前向传播（在GPU上算）\n",
    "    loss = loss_fn(y_hat, y) # 算损失\n",
    "    loss.backward()          # 反向传播\n",
    "    optimizer.step()         # 更新参数\n",
    "    optimizer.zero_grad()    # 清零梯度\n",
    "```\n",
    "\n",
    "### 权重初始化\n",
    "\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "```\n",
    "\n",
    "```\n",
    "对所有Linear层和Conv2d层用Xavier初始化\n",
    "之前学过的！apply会遍历所有层\n",
    "```\n",
    "\n",
    "### 完整训练代码\n",
    "\n",
    "```python\n",
    "# 超参数\n",
    "batch_size = 256\n",
    "lr = 0.9\n",
    "num_epochs = 10\n",
    "\n",
    "# 数据\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# 设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 初始化\n",
    "net.apply(init_weights)\n",
    "net.to(device)\n",
    "\n",
    "# 优化器和损失\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)     # 搬到GPU\n",
    "        y_hat = net(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 每个epoch结束后看看精度\n",
    "    # ...（评估代码）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、LeNet的设计思想（核心！）\n",
    "\n",
    "### 所有CNN都遵循的套路\n",
    "\n",
    "```\n",
    "通道数：越来越多    1 → 6 → 16 → ...\n",
    "空间大小：越来越小  28 → 14 → 5 → ...\n",
    "最后：拉平 + 全连接 → 输出类别数\n",
    "```\n",
    "\n",
    "**为什么这么设计？**\n",
    "\n",
    "```\n",
    "浅层：检测简单特征（边缘、纹理）→ 不需要很多通道\n",
    "深层：组合复杂特征（形状、物体）→ 需要更多通道\n",
    "\n",
    "空间变小：因为越往上看的范围越大\n",
    "通道变多：因为越往上特征种类越多\n",
    "\n",
    "就像：\n",
    "    看字：先看笔画（少数几种）→ 再看部首 → 再看整个字（种类很多）\n",
    "    空间越来越聚焦，但种类越来越多\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、关键要点\n",
    "\n",
    "### nn.Flatten()\n",
    "\n",
    "```python\n",
    "nn.Flatten()\n",
    "```\n",
    "\n",
    "```\n",
    "把多维数据拉成一维\n",
    "(批量, 16, 5, 5) → (批量, 400)\n",
    "\n",
    "为什么需要？\n",
    "    卷积层输出是4维的\n",
    "    全连接层只能接收2维的(批量, 特征数)\n",
    "    Flatten就是中间的桥梁\n",
    "```\n",
    "\n",
    "### Linear层的输入大小怎么算？\n",
    "\n",
    "```python\n",
    "nn.Linear(16 * 5 * 5, 120)\n",
    "#         ^^^^^^^^^^\n",
    "#         这个数怎么来的？\n",
    "\n",
    "# 就是Flatten前最后一层的输出：通道数 × 高 × 宽\n",
    "# 16 × 5 × 5 = 400\n",
    "\n",
    "# 不确定的话，用打印shape的方法：\n",
    "X = torch.rand(1, 1, 28, 28)\n",
    "for layer in net[:7]:       # 只跑到Flatten\n",
    "    X = layer(X)\n",
    "print(X.shape)              # torch.Size([1, 400])\n",
    "# 所以Linear的输入就是400\n",
    "```\n",
    "\n",
    "### GPU相关（只需要记这三行）\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)                     # 模型搬到GPU\n",
    "X, y = X.to(device), y.to(device)  # 数据搬到GPU\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 八、总结\n",
    "\n",
    "```\n",
    "LeNet结构：\n",
    "    Conv → Sigmoid → Pool → Conv → Sigmoid → Pool → Flatten → FC → FC → FC\n",
    "\n",
    "核心思想（所有CNN都一样）：\n",
    "    通道越来越多，空间越来越小，最后全连接输出\n",
    "\n",
    "代码要点：\n",
    "    1. nn.Flatten() 连接卷积和全连接\n",
    "    2. Linear的输入 = 最后卷积层的 通道×高×宽\n",
    "    3. 数据和模型都要.to(device)\n",
    "\n",
    "不确定输出形状？跑一下看看：\n",
    "    X = torch.rand(1, 1, 28, 28)\n",
    "    for layer in net:\n",
    "        X = layer(X)\n",
    "        print(layer.__class__.__name__, X.shape)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
