{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323fd8e3-a919-4a89-94df-77479c4fac9a",
   "metadata": {},
   "source": [
    "# Dropout丢弃法和Weight Decay的dist与理解\n",
    "\n",
    "---\n",
    "\n",
    "## 一、先搞懂\"它们在控制什么\"\n",
    "\n",
    "### 权重衰退 vs Dropout 的本质区别\n",
    "\n",
    "```\n",
    "权重衰退：\n",
    "  控制对象 → 权重 w 的大小\n",
    "  怎么控制 → 更新 w 的时候，先把 w 缩小一点\n",
    "  \n",
    "  类比：\n",
    "    w 就像汽车的油门踏板\n",
    "    权重衰退说：\"别踩太深，踩 80% 就够了\"\n",
    "    \n",
    "Dropout：\n",
    "  控制对象 → 神经元的输出\n",
    "  怎么控制 → 随机把一些输出变成 0\n",
    "  \n",
    "  类比：\n",
    "    神经元就像团队里的成员\n",
    "    Dropout 说：\"每次随机让一半人请假\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、为什么位置不一样？\n",
    "\n",
    "### 权重衰退在优化器里\n",
    "\n",
    "**因为它改的是\"更新权重的方式\"！**\n",
    "\n",
    "```python\n",
    "# 没有权重衰退的更新：\n",
    "w = w - lr × ∂L/∂w\n",
    "\n",
    "# 有权重衰退的更新：\n",
    "w = (1 - lr×λ) × w - lr × ∂L/∂w\n",
    "    ↑\n",
    "  先把 w 缩小一点！\n",
    "  \n",
    "这是\"更新规则\"的改变\n",
    "→ 更新规则由\"优化器\"控制\n",
    "→ 所以写在优化器里\n",
    "```\n",
    "\n",
    "**实际代码：**\n",
    "\n",
    "```python\n",
    "trainer = torch.optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=0.01,\n",
    "    weight_decay=0.01  # ← 告诉优化器：\"更新时顺便把 w 缩小\"\n",
    ")\n",
    "\n",
    "# 训练循环不用改\n",
    "for X, y in train_iter:\n",
    "    l = loss(net(X), y)  # 正常算损失\n",
    "    trainer.zero_grad()\n",
    "    l.backward()\n",
    "    trainer.step()       # ← 优化器在这一步自动做权重衰退,⭐⭐⭐说白了就是更新参数\n",
    "```\n",
    "\n",
    "**为什么不在模型里？**\n",
    "\n",
    "```\n",
    "权重衰退不改变数据流向\n",
    "它只改变\"参数怎么更新\"\n",
    "数据还是正常地流过模型：\n",
    "  输入 → Linear → ReLU → Linear → 输出\n",
    "  \n",
    "权重 w 在更新的时候才被缩小\n",
    "不是在前向传播时缩小\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Dropout 在模型里\n",
    "\n",
    "**因为它改的是\"数据的流动\"！**\n",
    "\n",
    "```python\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),      # ← 在这里截断数据流\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "```\n",
    "\n",
    "**数据流的变化：**\n",
    "\n",
    "```\n",
    "没有 Dropout：\n",
    "  输入 [256, 784]\n",
    "    ↓ Linear\n",
    "  [256, 256]\n",
    "    ↓ ReLU\n",
    "  [256, 256] = [0.5, -0.3, 0.8, 0.2, -0.1, ...]\n",
    "    ↓ 直接传给下一层\n",
    "  下一层收到：[0.5, -0.3, 0.8, 0.2, -0.1, ...]\n",
    "\n",
    "有 Dropout：\n",
    "  输入 [256, 784]\n",
    "    ↓ Linear\n",
    "  [256, 256]\n",
    "    ↓ ReLU\n",
    "  [256, 256] = [0.5, -0.3, 0.8, 0.2, -0.1, ...]\n",
    "    ↓ Dropout 把一半变成 0，剩下的 ×2\n",
    "  [256, 256] = [1.0,  0,  1.6,  0,  -0.2, ...]\n",
    "    ↓ 下一层收到的是被修改过的数据\n",
    "  下一层收到：[1.0, 0, 1.6, 0, -0.2, ...]  ← 不一样！\n",
    "```\n",
    "\n",
    "**Dropout 像一个\"过滤器\"：**\n",
    "\n",
    "```\n",
    "Linear → ReLU → [Dropout 在这截断] → Linear\n",
    "                  ↑\n",
    "            数据流过这里时被改变\n",
    "            必须放在数据流动的路径上\n",
    "            → 所以在模型里\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、用生活例子对比\n",
    "\n",
    "### 权重衰退 = 限制工具的力度\n",
    "\n",
    "```\n",
    "你有一把电钻（模型）\n",
    "钻头的转速（权重 w）可以调\n",
    "\n",
    "权重衰退说：\n",
    "  \"转速别开太猛，开到 80% 就够了\"\n",
    "  \n",
    "这是在调整\"工具的使用方式\"\n",
    "→ 对应\"优化器\"（控制怎么更新参数）\n",
    "\n",
    "你不需要在电钻上加零件\n",
    "只需要改变\"怎么用它\"\n",
    "```\n",
    "\n",
    "### Dropout = 随机拆掉一些零件\n",
    "\n",
    "```\n",
    "你有一辆车（模型）\n",
    "车上有 4 个轮子（4 个神经元）\n",
    "\n",
    "Dropout 说：\n",
    "  \"每次开车前，随机卸掉 2 个轮子\"\n",
    "  \n",
    "这是在改变\"车的结构\"\n",
    "→ 对应\"模型本身\"（改变数据流）\n",
    "\n",
    "你必须真的在车上动手脚\n",
    "不能只是\"改变开车方式\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、为什么预测不用 Dropout？\n",
    "\n",
    "### 用最简单的例子\n",
    "\n",
    "假设你是一个学生（模型），要参加考试（预测）。\n",
    "\n",
    "#### 训练阶段（平时学习）\n",
    "\n",
    "```\n",
    "老师用 Dropout 训练你：\n",
    "\n",
    "第1天：\n",
    "  老师说：\"今天不许用左手\"\n",
    "  你只能用右手写字、吃饭、做题\n",
    "  → 你学会了\"不依赖左手也能学习\"\n",
    "\n",
    "第2天：\n",
    "  老师说：\"今天不许用右手\"\n",
    "  你只能用左手\n",
    "  → 你学会了\"不依赖右手也能学习\"\n",
    "\n",
    "第3天：\n",
    "  老师说：\"今天两只手都能用，但不许用左眼\"\n",
    "  → 你学会了\"不依赖某个特定能力\"\n",
    "\n",
    "...\n",
    "\n",
    "结果：\n",
    "  你变成了\"全能选手\"\n",
    "  不会过度依赖某一个技能\n",
    "```\n",
    "\n",
    "#### 预测阶段（最后的考试）\n",
    "\n",
    "```\n",
    "考试的时候：\n",
    "\n",
    "❌ 错误做法（继续用 Dropout）：\n",
    "  考试时老师说：\"继续随机禁用你的某些能力\"\n",
    "  → 你：？？？我要考试啊！\n",
    "  → 有时候左手被禁，有时候右手被禁\n",
    "  → 每次考试成绩都不一样（随机的）\n",
    "  → 这不合理！\n",
    "\n",
    "✅ 正确做法（不用 Dropout）：\n",
    "  考试时老师说：\"发挥你的全部能力吧\"\n",
    "  → 你用上所有技能\n",
    "  → 因为你平时被强制\"不依赖某个技能\"训练过\n",
    "  → 现在用全部技能，效果更好\n",
    "  → 每次考试成绩稳定\n",
    "```\n",
    "\n",
    "### 用代码对比\n",
    "\n",
    "```python\n",
    "# 训练时（用 Dropout）\n",
    "net.train()  # 开启训练模式\n",
    "\n",
    "for X, y in train_iter:\n",
    "    output = net(X)\n",
    "    # 每次前向传播，Dropout 都随机关掉一些神经元\n",
    "    # 第1批：关掉 [1, 3, 5]\n",
    "    # 第2批：关掉 [2, 4]\n",
    "    # 第3批：关掉 [1, 2, 3]\n",
    "    # ...\n",
    "    \n",
    "    # 这让模型学会\"不依赖某些固定的神经元\"\n",
    "```\n",
    "\n",
    "```python\n",
    "# 预测时（不用 Dropout）\n",
    "net.eval()  # 开启评估模式\n",
    "\n",
    "output = net(X_test)\n",
    "# 所有神经元都工作\n",
    "# 每次输入同样的 X，输出都一样（确定性）\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "### 更直观的例子\n",
    "\n",
    "```\n",
    "训练：\n",
    "  就像你投篮练习\n",
    "  教练让你：\n",
    "    第1次只用左手投 10 次\n",
    "    第2次只用右手投 10 次\n",
    "    第3次只用左手投 10 次\n",
    "    ...\n",
    "  \n",
    "  这让你左右手都练得很均衡\n",
    "\n",
    "比赛（预测）：\n",
    "  你当然要用双手投篮！\n",
    "  不会傻到继续\"只用左手\"或\"只用右手\"\n",
    "  \n",
    "  用双手 = 训练时\"左手\"和\"右手\"效果的平均\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 完整对比表\n",
    "\n",
    "| | 权重衰退 | Dropout |\n",
    "|---|---|---|\n",
    "| **控制什么** | 权重 w 的大小 | 神经元的输出 |\n",
    "| **怎么控制** | 更新 w 时先缩小 | 前向传播时随机置 0 |\n",
    "| **改变什么** | 参数更新方式 | 数据流动 |\n",
    "| **写在哪里** | 优化器里 | 模型里 |\n",
    "| **训练时** | 每次更新 w 都缩小 | 每次前向传播都随机 |\n",
    "| **预测时** | 不做任何事 | 不做任何事 |\n",
    "| **为什么预测不用** | 权重已经训练好了 | 要确定的输出，不能随机 |\n",
    "\n",
    "---\n",
    "\n",
    "## 代码完整示例\n",
    "\n",
    "```python\n",
    "# ========== 搭建模型 ==========\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),      # ← Dropout 在这（模型里）\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "# ========== 创建优化器 ==========\n",
    "trainer = torch.optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=0.01,\n",
    "    weight_decay=0.01     # ← 权重衰退在这（优化器里）\n",
    ")\n",
    "\n",
    "# ========== 训练 ==========\n",
    "net.train()  # 开启 Dropout\n",
    "for X, y in train_iter:\n",
    "    l = loss(net(X), y)\n",
    "    trainer.zero_grad()\n",
    "    l.backward()\n",
    "    trainer.step()        # 权重衰退在这一步生效\n",
    "\n",
    "# ========== 预测 ==========\n",
    "net.eval()   # 关闭 Dropout\n",
    "output = net(X_test)      # 得到确定的输出\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 自查题\n",
    "\n",
    "```\n",
    "① 权重衰退写在哪里？为什么？\n",
    "   → 优化器里，因为它改变\"参数更新方式\"\n",
    "\n",
    "② Dropout 写在哪里？为什么？\n",
    "   → 模型里，因为它改变\"数据流动\"\n",
    "\n",
    "③ 预测时用权重衰退吗？\n",
    "   → 不用，权重已经训练好了\n",
    "\n",
    "④ 预测时用 Dropout 吗？\n",
    "   → 不用，要确定的输出\n",
    "\n",
    "⑤ 如果预测时继续用 Dropout 会怎样？\n",
    "   → 每次输出都不一样（随机的），不合理\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8f255-e152-4fa2-be64-8dce3ed3e2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
