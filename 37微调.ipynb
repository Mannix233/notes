{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdda5bb-8fff-4820-a66e-5a82ec72aaee",
   "metadata": {},
   "source": [
    "# 微调（Fine-Tuning）\n",
    "\n",
    "---\n",
    "\n",
    "## 一、微调是什么？\n",
    "\n",
    "> 别人在大数据集上训练好了一个模型，拿过来，改一改最后一层，在自己的小数据集上接着训练。\n",
    "\n",
    "**为什么这样做？**\n",
    "\n",
    "- 你自己的数据通常很少（比如5万张、100类）\n",
    "- 从零训练一个好模型需要海量数据（ImageNet有120万张）\n",
    "- 别人训练好的模型已经学会了\"看图片\"的能力，你只需要在此基础上微调\n",
    "\n",
    "---\n",
    "\n",
    "## 二、神经网络的两部分\n",
    "\n",
    "把任何一个分类网络想成两块：\n",
    "\n",
    "```\n",
    "图片 → [特征提取（很多层）] → [分类器（最后一层全连接）] → 预测结果\n",
    "```\n",
    "\n",
    "| 部分 | 作用 | 微调时怎么处理 |\n",
    "|:--|:--|:--|\n",
    "| 特征提取（前面所有层） | 把像素变成有意义的特征 | **从预训练模型复制过来**（不是随机初始化） |\n",
    "| 分类器（最后一层FC） | 根据特征判断类别 | **随机初始化**（因为你的类别和ImageNet不同） |\n",
    "\n",
    "**核心想法：** 特征提取的能力是通用的（边缘、纹理、形状……），换个数据集也能用。但分类器是跟类别绑定的，必须重新来。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、微调 vs 从零训练\n",
    "\n",
    "| | 微调 | 从零训练 |\n",
    "|:--|:--|:--|\n",
    "| 特征提取层初始化 | 用预训练权重 | 随机 |\n",
    "| 最后一层初始化 | 随机（类别数不同） | 随机 |\n",
    "| 学习率 | 小（已经接近最优了） | 正常大小 |\n",
    "| 训练轮数 | 少（1~5个epoch） | 多（几十上百个epoch） |\n",
    "| 最终精度（热狗数据集） | **~94%** | ~84% |\n",
    "\n",
    "**就改了一个地方：初始化用预训练权重而不是随机，精度差了10个点。**\n",
    "\n",
    "---\n",
    "\n",
    "## 四、代码实现（逐行解释）\n",
    "\n",
    "### 第1步：数据准备\n",
    "\n",
    "```python\n",
    "# 这里的 normalize 必须和 ImageNet 预训练时用的一样\n",
    "# 因为预训练模型\"习惯\"了这种输入分布\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],   # ImageNet 三个通道的均值\n",
    "    std=[0.229, 0.224, 0.225]     # ImageNet 三个通道的标准差\n",
    ")\n",
    "```\n",
    "\n",
    "> **为什么测试也要 normalize？** 不是为了增广，而是预训练模型训练时输入就是 normalize 过的。你喂原始数据进去，它\"看不懂\"。\n",
    "\n",
    "```python\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),     # ImageNet模型要求224×224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,                              # 和预训练保持一致\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),        # 短边缩放到256，长边等比例跟着变\n",
    "    transforms.CenterCrop(224),    # 从中心裁出224×224\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('data/hotdog/train', transform=train_transform)\n",
    "test_dataset  = datasets.ImageFolder('data/hotdog/test',  transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 第2步：加载预训练模型并修改最后一层\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# pretrained=True：不仅下载模型结构，还下载在ImageNet上训练好的权重\n",
    "pretrained_net = models.resnet18(pretrained=True)\n",
    "```\n",
    "\n",
    "> **`pretrained=True` vs `pretrained=False`：**\n",
    "> - `True`：权重是ImageNet上训练好的（微调用这个）\n",
    "> - `False`：权重是随机初始化的（从零训练用这个）\n",
    "\n",
    "```python\n",
    "# 看一下最后一层长什么样\n",
    "print(pretrained_net.fc)\n",
    "# Linear(in_features=512, out_features=1000, bias=True)\n",
    "# 输入512维特征，输出1000类（ImageNet的1000个类别）\n",
    "```\n",
    "\n",
    "```python\n",
    "# 我们的任务只有2类（热狗 / 不是热狗），所以要替换最后一层\n",
    "pretrained_net.fc = nn.Linear(512, 2)\n",
    "\n",
    "# 只对新的最后一层做随机初始化\n",
    "nn.init.xavier_uniform_(pretrained_net.fc.weight)\n",
    "```\n",
    "\n",
    "> **`nn.Linear(512, 2)`：** 创建一个新的全连接层，输入512维，输出2类。这一层的权重是随机的，前面所有层的权重还是ImageNet预训练的。\n",
    "\n",
    "> **`nn.init.xavier_uniform_()`：** 一种比较好的随机初始化方法，比默认的随机效果好一点。`_` 结尾表示原地修改。\n",
    "\n",
    "**执行完这两行后，模型的状态：**\n",
    "```\n",
    "前面所有层 → ImageNet预训练权重（已经很好了）\n",
    "最后一层FC → 随机初始化（需要重新学）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 第3步：设置不同的学习率\n",
    "\n",
    "**关键技巧：** 前面的层已经很好了，用小学习率微调；最后一层是随机的，用大学习率快速学。\n",
    "\n",
    "```python\n",
    "# 把参数分成两组\n",
    "params_1x = [p for name, p in pretrained_net.named_parameters()\n",
    "             if 'fc' not in name]   # 不包含'fc'的 = 前面所有层\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': params_1x, 'lr': 5e-5},              # 前面的层：小学习率\n",
    "    {'params': pretrained_net.fc.parameters(), 'lr': 5e-4},  # 最后一层：10倍学习率\n",
    "], momentum=0.9, weight_decay=0.001)\n",
    "```\n",
    "\n",
    "> **语法解释：**\n",
    "> - `named_parameters()`：返回 `(参数名, 参数值)` 的迭代器\n",
    "> - `'fc' not in name`：名字里不包含 `'fc'` 的参数 = 除最后一层外的所有参数\n",
    "> - SGD 的参数列表可以传**字典列表**，每组参数用不同的学习率\n",
    "\n",
    "---\n",
    "\n",
    "### 第4步：训练（和之前完全一样）\n",
    "\n",
    "```python\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 5   # 微调只需要很少的epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pretrained_net.train()\n",
    "    for X, y in train_loader:\n",
    "        output = pretrained_net(X)\n",
    "        loss = loss_fn(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "**训练代码和从零训练一模一样，区别全在前面的初始化和学习率设置上。**\n",
    "\n",
    "---\n",
    "\n",
    "## 五、效果对比\n",
    "\n",
    "| | 微调（pretrained=True） | 从零训练（pretrained=False） |\n",
    "|:--|:--|:--|\n",
    "| epoch 1 精度 | 已经很高了 | 还在慢慢爬 |\n",
    "| epoch 5 精度 | **~94%** | ~84% |\n",
    "| 是否还需要更多epoch | 不需要，1-2个就够了 | 还在涨，但很慢 |\n",
    "\n",
    "---\n",
    "\n",
    "## 六、整体流程总结\n",
    "\n",
    "```\n",
    "微调三步：\n",
    "  1. 加载预训练模型        pretrained=True\n",
    "  2. 替换最后一层          fc = nn.Linear(512, 你的类别数)\n",
    "  3. 分组设学习率          前面的层用小学习率，最后一层用大学习率\n",
    "\n",
    "其余（数据加载、训练循环、loss计算）和从零训练完全相同。\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、实际建议\n",
    "\n",
    "**几乎所有情况下，都应该从微调开始，而不是从零训练。**\n",
    "\n",
    "- 数据少 → 微调效果远好于从零训练\n",
    "- 数据多 → 微调至少不会更差，而且收敛更快\n",
    "- 从零训练大模型是大公司/学术界的事，个人和应用层面用微调就够了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e683c2-1cc0-4f06-b16b-b0aa2fa1e85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
