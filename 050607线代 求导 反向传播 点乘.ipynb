{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a26b659-c92a-42ed-ab58-78a9734a2115",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. `requires_grad`：是否追踪梯度（Autograd 开关）\n",
    "\n",
    "在 PyTorch 中，`requires_grad` 是 Tensor 的核心属性，用来标记**是否需要对该张量计算梯度**。\n",
    "\n",
    "### 1.1 开启梯度追踪：`requires_grad=True`\n",
    "\n",
    "- 当 `requires_grad=True` 时，PyTorch 会在**前向传播**中记录该张量参与的运算，构建**计算图（computation graph）**。\n",
    "- 之后调用 `backward()` 时，能够沿计算图自动求导，得到梯度。\n",
    "\n",
    "示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.randn((), requires_grad=True)\n",
    "d = a * a + 1\n",
    "d.backward()\n",
    "print(a.grad)  # dd/da\n",
    "```\n",
    "\n",
    "### 1.2 关闭梯度追踪：`requires_grad=False`（默认）\n",
    "\n",
    "- 不记录计算图，也不计算梯度，节省内存与计算。\n",
    "- 常用于：输入数据、固定参数、推理阶段等。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `.backward()` / `.grad` / `zero_()`：Autograd 三个基本动作\n",
    "\n",
    "### 2.1 `x.grad.zero_()`：清空梯度（防止梯度累加）\n",
    "\n",
    "PyTorch 默认会**累加**梯度：如果不清零，下一次 `backward()` 得到的梯度会加到旧梯度上。\n",
    "\n",
    "```python\n",
    "x.grad.zero_()\n",
    "```\n",
    "\n",
    "训练中更常见的是：\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "### 2.2 `y.backward()`：反向传播计算梯度\n",
    "\n",
    "- `backward()` 会沿计算图反向传播，计算所有 `requires_grad=True` 的叶子张量的梯度。\n",
    "- 结果一般存放在对应张量的 `.grad` 中。\n",
    "\n",
    "### 2.3 `x.grad`：查看梯度\n",
    "\n",
    "- `.grad` 的形状通常与 `x` 的形状一致（“梯度形状 = 参数形状”）。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. “非标量怎么求导”：`sum()`（或 `mean()`）把输出变成标量\n",
    "\n",
    "PyTorch 中最常用的方式是：**把向量/矩阵输出先聚合成标量**再反传。\n",
    "\n",
    "### 3.1 `sum()` 求导示例：`y = x.sum()`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4.0, requires_grad=True)  # tensor([0.,1.,2.,3.])\n",
    "y = x.sum()                                # 标量\n",
    "y.backward()\n",
    "print(x.grad)  # tensor([1., 1., 1., 1.])\n",
    "```\n",
    "\n",
    "### 3.2 “求导的求和技巧”：`y = x*x` 但用 `y.sum().backward()`\n",
    "\n",
    "```python\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "x.grad.zero_()\n",
    "\n",
    "y = x * x                 # tensor([0.,1.,4.,9.])  (非标量)\n",
    "y.sum().backward()        # 关键：先 sum 成标量再 backward\n",
    "print(x.grad)             # tensor([0., 2., 4., 6.])  -> 2x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `detach()`：把某些计算“移到计算图之外”（切断梯度流）\n",
    "\n",
    "### 4.1 核心含义\n",
    "\n",
    "`u = y.detach()` 的意思：\n",
    "\n",
    "- `u` **拿到 `y` 当前的数值**；\n",
    "- 但 `u` **不再连接到产生 `y` 的计算图**；\n",
    "- 因此从 `u` 出发的后续计算，梯度不会回传到 `y` 的来源（链式法则在此断开）。\n",
    "\n",
    "### 4.2 为什么用了 `detach()` 结果会变？\n",
    "\n",
    "对比两种情况（非常常见的 D2L 讲法）：\n",
    "\n",
    "#### 情况 A：使用 `detach()`\n",
    "\n",
    "```python\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "x.grad.zero_()\n",
    "\n",
    "y = x * x          # y 与 x 有计算图关系\n",
    "u = y.detach()     # u 与 y 断开：u 被当作常量（对 x 不再可导）\n",
    "z = u * x          # z = (常量u) * x\n",
    "\n",
    "z.sum().backward()\n",
    "print(\"x.grad =\", x.grad)\n",
    "print(\"u      =\", u)\n",
    "```\n",
    "\n",
    "- 因为 `u` 被视作常量，`z = u*x` 对 `x` 的导数就是 `u`（逐元素）。\n",
    "\n",
    "#### 情况 B：不使用 `detach()`\n",
    "\n",
    "```python\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "x.grad.zero_()\n",
    "\n",
    "y = x * x\n",
    "z = y * x          # z = x^2 * x = x^3\n",
    "\n",
    "z.sum().backward()\n",
    "print(x.grad)      # 逐元素应为 3*x^2\n",
    "```\n",
    "\n",
    "### 4.3 典型用途（D2L 常见语境）\n",
    "\n",
    "- 固定某一部分计算/参数，不希望梯度更新它；\n",
    "- 只想用某个中间结果的“当前值”，但不想让梯度穿过该分支。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. `sum(axis/dim=...)` 的维度变化 + `keepdims/keepdim` 的作用\n",
    "\n",
    "这部分是 D2L 里非常核心的“维度直觉”。\n",
    "\n",
    "### 5.1 二维张量：`(5,4)` 的例子\n",
    "\n",
    "假设 `A.shape = (5,4)`：\n",
    "\n",
    "- `A.sum(axis=0)`：对 **列** 求和（沿行方向累加），结果形状通常是 `(4,)`\n",
    "- `A.sum(axis=1)`：对 **行** 求和（沿列方向累加），结果形状通常是 `(5,)`\n",
    "\n",
    "在 PyTorch 中写法通常是：\n",
    "\n",
    "- `A.sum(dim=0)` 或 `A.sum(dim=1)`\n",
    "\n",
    "### 5.2 `keepdims/keepdim`：求和后是否保留被压缩的维度\n",
    "\n",
    "- 默认 `keepdim=False`：被求和的那一维会**消失**。\n",
    "- 若 `keepdim=True`：那一维会保留为长度为 1 的维度，方便后续广播对齐。\n",
    "\n",
    "例：`A.shape = (5,4)`\n",
    "\n",
    "- `A.sum(dim=1)` -> shape `(5,)`\n",
    "- `A.sum(dim=1, keepdim=True)` -> shape `(5,1)`\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 广播（Broadcasting）与“行归一化”：为什么常常需要 `keepdim=True`\n",
    "\n",
    "### 6.1 正确的逐行归一化写法\n",
    "\n",
    "目标：每一行除以该行的和，使每行元素和为 1。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([[1., 3.],\n",
    "                  [2., 4.]])\n",
    "\n",
    "sum_A = A.sum(dim=1, keepdim=True)   # shape (2,1), 值 [[4],[6]]\n",
    "normalized = A / sum_A\n",
    "\n",
    "print(sum_A)\n",
    "print(normalized)\n",
    "```\n",
    "\n",
    "输出应类似：\n",
    "\n",
    "- `sum_A = [[4.],[6.]]`\n",
    "- `normalized = [[0.25, 0.75], [0.3333, 0.6667]]`（每行和为 1）\n",
    "\n",
    "### 6.2 不用 `keepdim=True` 可能产生“错位广播”\n",
    "\n",
    "```python\n",
    "sum_A_bad = A.sum(dim=1)   # shape (2,) -> [4,6]\n",
    "normalized_bad = A / sum_A_bad\n",
    "print(normalized_bad)\n",
    "```\n",
    "\n",
    "这种写法会触发广播，但广播对齐方式可能导致结果**不是逐行除以本行和**（容易出现错位/逻辑错误），因此做“按行/按列归一化”时，D2L 强烈建议用 `keepdim=True` 保持维度对齐。\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 三维张量的 `sum`：`shape = (2,5,4)` 时 `axis/dim` 怎么理解\n",
    "\n",
    "设 `a = torch.ones((2,5,4))`：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "a = torch.ones((2,5,4))\n",
    "print(a.shape)                 # torch.Size([2, 5, 4])\n",
    "\n",
    "b = a.sum(dim=1)\n",
    "print(b.shape)                 # torch.Size([2, 4])\n",
    "print(b)                       # 每个元素为 5（因为 dim=1 那一维长度是 5）\n",
    "```\n",
    "\n",
    "- `sum(dim=1)`：把中间那一维（长度 5）求和掉，所以从 `(2,5,4)` 变成 `(2,4)`\n",
    "- 若 `keepdim=True`：\n",
    "  - `a.sum(dim=1, keepdim=True)` 形状为 `(2,1,4)`\n",
    "- 多维一起求和（概念上与 D2L 一致）：\n",
    "  - `a.sum(dim=(1,2), keepdim=True)` 形状为 `(2,1,1)`\n",
    "\n",
    "---\n",
    "\n",
    "## 8. `cumsum`：沿某个轴计算“累积和”（cumulative sum）\n",
    "\n",
    "`cumsum` 会从起点开始，把当前元素与之前元素不断累加。\n",
    "\n",
    "### 8.1 `axis/dim=0` 的含义\n",
    "\n",
    "- `dim=0`：沿“行”方向累积（对每一列分别做从上到下的累加）。\n",
    "\n",
    "示例（5×4）：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "A = torch.arange(20.0).reshape(5,4)\n",
    "print(A)\n",
    "\n",
    "C = A.cumsum(dim=0)\n",
    "print(C)\n",
    "```\n",
    "\n",
    "直观理解（按列累加）：\n",
    "\n",
    "- 第 1 行：不变\n",
    "- 第 2 行：第1行 + 第2行\n",
    "- 第 3 行：前两行累计 + 第3行\n",
    "- ...\n",
    "- 最后一行：就是该列的总累积结果\n",
    "\n",
    "---\n",
    "\n",
    "## 9. 行向量 vs 列向量：以及 `torch.mv` 的“向量隐形处理”\n",
    "\n",
    "### 9.1 行/列向量概念\n",
    "\n",
    "- 行向量：形状 `1×n`\n",
    "- 列向量：形状 `n×1`\n",
    "- 一维张量 `x.shape == (n,)` 在不同算子里可能有约定处理。\n",
    "\n",
    "### 9.2 `torch.mv(A, x)`：矩阵 × 向量\n",
    "\n",
    "- `torch.mv` 专门用于 **(m×n) 矩阵** 乘 **(n,) 向量**。\n",
    "- 你看到 `x.shape=(n,)` 虽是 1D，但在 `mv` 语义里会按“向量”规则处理，使维度匹配 `A @ x`。\n",
    "\n",
    "示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "A = torch.randn(5,4)\n",
    "x = torch.randn(4)          # shape (4,)\n",
    "b = torch.mv(A, x)          # shape (5,)\n",
    "print(b.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. `torch.mm(A, B)`：矩阵 × 矩阵（2D × 2D）\n",
    "\n",
    "`torch.mm` 是 **矩阵乘法**（matrix-matrix multiply）：\n",
    "\n",
    "- 输入必须都是 2D\n",
    "- 维度规则：`A(m×n) @ B(n×p) -> C(m×p)`\n",
    "\n",
    "示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20.0).reshape(5,4)   # 5×4\n",
    "B = torch.ones(4,3)                   # 4×3\n",
    "C = torch.mm(A, B)                    # 5×3\n",
    "print(C.shape)\n",
    "print(C)\n",
    "```\n",
    "\n",
    "如果 `B` 每列都是全 1，则 `C` 的每个元素是“对应行元素之和”（并在每列重复）。\n",
    "\n",
    "---\n",
    "\n",
    "## 11. 建议练习（与截图一致）：把 `x.sum()` 升级为 `(x*x).sum()`\n",
    "\n",
    "目的：亲手验证梯度从 `1` 变成 `2x`。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "y = (x * x).sum()\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)   # 应为 2*x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. 本章（对应 D2L 语境）你需要真正“吃透”的东西（不靠背公式）\n",
    "\n",
    "- 会用：`requires_grad=True`\n",
    "- 会用：`backward()`\n",
    "- 记得：梯度会累加 -> 每轮先 `zero_grad()` / `grad.zero_()`\n",
    "- 非标量输出：常用 `sum()` / `mean()` 变标量再反传\n",
    "- 需要“断梯度”：`detach()`\n",
    "- 维度/广播：`sum(dim=..., keepdim=True)` 是很多归一化/广播正确性的关键\n",
    "- 线代接口区分：`mv`（矩阵×向量） vs `mm`（矩阵×矩阵）\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7c1ab-f382-4c15-935f-354fcc36c553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
