{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef2d2da-d272-4e98-9296-5adc0fd3dcc1",
   "metadata": {},
   "source": [
    "# 数值稳定性\n",
    "\n",
    "## 一、比喻\n",
    "\n",
    "先用一个生活例子理解  \n",
    "想象你在玩传话游戏：  \n",
    "第1个人 → 第2个人 → … → 第100个人  \n",
    "每个人传话时，要么会\"夸大\"一点，要么会\"缩小\"一点  \n",
    "\n",
    "- 每个人都稍微夸大一点（×1.5）：传100次后，原话被放大到离谱 → **梯度爆炸**  \n",
    "- 每个人都稍微缩小一点（×0.8）：传100次后，原话几乎听不见了 → **梯度消失**\n",
    "\n",
    "神经网络的梯度反向传播就是这个传话过程！\n",
    "\n",
    "### 核心原理\n",
    "\n",
    "神经网络的梯度 = 一堆矩阵连乘  \n",
    "网络有d层：  \n",
    "梯度 = ∂L/∂h_d × ∂h_d/∂h_(d-1) × ... × ∂h_(t+1)/∂h_t × ∂h_t/∂w_t  \n",
    "(d-t) 个矩阵相乘\n",
    "\n",
    "问题就出在这里：你要把很多矩阵乘在一起  \n",
    "\n",
    "比如：  \n",
    "\n",
    "- 1.5 × 1.5 × 1.5 × ... （100次）= 4 × 10^17 → **爆炸**  \n",
    "- 0.8 × 0.8 × 0.8 × ... （100次）= 2 × 10^-10 → **消失**\n",
    "\n",
    "## 二、梯度爆炸与梯度消失详解\n",
    "\n",
    "### 梯度爆炸 —— 数越乘越大\n",
    "\n",
    "**什么时候会发生？**  \n",
    "当每层梯度的值 > 1 时，层数一多，连乘就会导致数值急剧增大，最终引发梯度爆炸。\n",
    "\n",
    "**以 ReLU 激活函数为例：**  \n",
    "\n",
    "- ReLU 的导数 = **1**（输入 > 0 时）或 **0**（输入 ≤ 0 时）  \n",
    "- 反向传播中的梯度 ≈ $ W_1 \\times W_2 \\times W_3 \\times \\cdots $ 的部分元素相乘  \n",
    "- 如果权重矩阵中的值普遍略大于 1 → 连乘后结果呈指数级增长 → **爆炸**\n",
    "\n",
    "**会带来什么问题？**  \n",
    "\n",
    "- **问题1：数值变成 inf（无穷大）**  \n",
    "  → 尤其在使用 **16位浮点数**（FP16）时，数值范围仅为 -65504 到 +65504  \n",
    "  → 一旦超出该范围，数值变为 `inf`，程序直接崩溃  \n",
    "\n",
    "- **问题2：学习率极难调节**  \n",
    "  → 学习率稍大 → 参数更新幅度过大 → 梯度进一步放大 → 引发爆炸  \n",
    "  → 学习率过小 → 参数几乎不更新 → 训练停滞  \n",
    "  → 只有非常狭窄的学习率区间可用，调参极其困难\n",
    "\n",
    "### 梯度消失 —— 数越乘越小\n",
    "\n",
    "**什么时候会发生？**  \n",
    "当每层梯度的值 < 1 时，随着网络层数增加，多个小于1的数连续相乘，结果趋近于零，造成梯度消失。\n",
    "\n",
    "**以 Sigmoid 激活函数为例：**  \n",
    "\n",
    "- Sigmoid 函数：$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $  \n",
    "- 其导数：$ \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) $\n",
    "\n",
    "| 特性 | 说明 |\n",
    "| --- | --- |\n",
    "| **Sigmoid 导数的最大值** | **只有 0.25** |\n",
    "| **输入稍大（如 > 4）时** | 导数接近 0 |\n",
    "\n",
    "因此，每一层的梯度都小于 0.25，经过多层连乘：\n",
    "$$\n",
    "0. 25 \\times 0.25 \\times 0.25 \\times \\cdots \\text{（100次）} \\approx 0\n",
    "$$\n",
    "\n",
    "**会带来什么问题？**  \n",
    "\n",
    "- **底层（靠近输入的层）根本学不动！**  \n",
    "  反向传播是从输出层向输入层传递梯度的过程：\n",
    "\n",
    "| 层级位置 | 梯度状态 | 学习能力 |\n",
    "| --- | --- | --- |\n",
    "| 顶层（靠近输出） | 梯度正常 ✅ | 正常更新参数 |\n",
    "| 中间层 | 梯度开始变小 ⚠️ | 更新缓慢 |\n",
    "| 底层（靠近输入） | 梯度 ≈ 0 ❌ | 完全无法学习 |\n",
    "\n",
    "**结果：**  \n",
    "你以为你搭建了一个100层的深度神经网络，  \n",
    "但实际上只有顶部几层在学习，  \n",
    "底层参数几乎不变，等同于一个浅层网络 —— **白搭了！**\n",
    "\n",
    "### 一张图总结\n",
    "\n",
    "| 梯度爆炸 | 梯度消失 |\n",
    "| --- | --- |\n",
    "| 原因：每层梯度 > 1，连乘后 → 越乘越大 | 原因：每层梯度 < 1，连乘后 → 越乘越小 |\n",
    "| 激活函数举例：ReLU（当权重 > 1 时） | 激活函数举例：Sigmoid（导数最大仅 0.25） |\n",
    "| 后果：数值溢出为 inf，程序崩溃 | 后果：底层梯度为 0，无法更新参数 |\n",
    "| 学习率极难调 | 即使增大学习率也无济于事 |\n",
    "| 网络越深越严重 | 网络越深越严重 |\n",
    "\n",
    "### 你需要记住的\n",
    "\n",
    "#### 🔴 必须记住\n",
    "\n",
    "| 要点 | 内容 |\n",
    "| --- | --- |\n",
    "| **核心概念** | 梯度爆炸和梯度消失是深度网络训练的两大障碍 |\n",
    "| **根本原因** | 反向传播本质是多个矩阵连乘，导致数值要么不断放大，要么不断缩小 |\n",
    "| **Sigmoid 的坑** | 导数最大只有 0.25，深层网络中必然导致梯度消失，现已基本被淘汰 |\n",
    "| **ReLU 为什么好** | 导数为 1 或 0，在正区域能保持梯度不衰减，有效缓解梯度消失 |\n",
    "| **实战指导** | 优先选用 ReLU 类激活函数，权重需合理初始化（如 Xavier、He 初始化） |\n",
    "\n",
    "-----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7dbed-23c5-4754-8c7a-2bfa986af60b",
   "metadata": {},
   "source": [
    "-----\n",
    "# 权重初始化\n",
    "## 🔴 必须记住的\n",
    "\n",
    "### 1. 两个核心问题\n",
    "\n",
    "```\n",
    "深度网络的梯度 = 很多矩阵连乘\n",
    "\n",
    "┌─────────────────────────────────────────────┐\n",
    "│                                             │\n",
    "│  梯度爆炸：每层梯度 > 1，连乘后 → 超级大💥   │\n",
    "│    后果：程序崩溃 / 学习率极难调              │\n",
    "│                                             │\n",
    "│  梯度消失：每层梯度 < 1，连乘后 → 接近0👻    │\n",
    "│    后果：底层学不动，深网络等于白搭           │\n",
    "│                                             │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 2. 三个解决方法（记名字和思想）\n",
    "\n",
    "```\n",
    "方法1：乘法变加法（后面会学）\n",
    "  ├── ResNet（图像领域）  ← 后面会详细学\n",
    "  └── LSTM（序列领域）    ← 后面会详细学\n",
    "  思想：100次乘法容易爆炸/消失，换成加法就稳定了\n",
    "\n",
    "方法2：归一化\n",
    "  └── 把梯度/输出强行拉回\"均值0，方差1\"\n",
    "  \n",
    "方法3：合理的权重初始化 + 激活函数 ← 14节的重点\n",
    "```\n",
    "\n",
    "### 3. Xavier初始化（必须记住怎么用）\n",
    "\n",
    "```python\n",
    "# 核心思想：权重的方差要根据输入输出维度来定\n",
    "\n",
    "# 正态分布版本：\n",
    "# 方差 = 2 / (输入维度 + 输出维度)\n",
    "\n",
    "# PyTorch中直接用：\n",
    "nn.init.xavier_normal_(m.weight)   # 正态分布版\n",
    "nn.init.xavier_uniform_(m.weight)  # 均匀分布版\n",
    "```\n",
    "\n",
    "**为什么不能用固定的0.01？**\n",
    "\n",
    "```\n",
    "固定方差0.01：\n",
    "  → 小网络可能没问题\n",
    "  → 深网络就不行了，因为没考虑每层的输入输出维度\n",
    "\n",
    "Xavier初始化：\n",
    "  → 根据每层的输入和输出维度自动调整方差\n",
    "  → 让每层的输出和梯度都保持在合理范围\n",
    "```\n",
    "\n",
    "### 4. 激活函数怎么选\n",
    "\n",
    "```\n",
    "✅ ReLU：   在零点附近 f(x)≈x，满足要求，最常用\n",
    "✅ tanh：   在零点附近 f(x)≈x，满足要求\n",
    "⚠️ Sigmoid：不过原点（f(0)=0.5不是0），需要调整才行\n",
    "\n",
    "实际工作中：默认用ReLU就对了\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🟡 理解思想就行（不用背具体公式）\n",
    "\n",
    "### 核心思想：让每层的输出和梯度都\"差不多大\"\n",
    "\n",
    "```\n",
    "理想状态：不管网络多深\n",
    "\n",
    "  第1层输出：均值=0，方差=固定值\n",
    "  第2层输出：均值=0，方差=固定值\n",
    "  ...\n",
    "  第100层输出：均值=0，方差=固定值\n",
    "\n",
    "  梯度也一样：每层梯度的均值=0，方差=固定值\n",
    "\n",
    "这样就不会爆炸也不会消失了！\n",
    "```\n",
    "\n",
    "### Xavier怎么推出来的（理解逻辑）\n",
    "\n",
    "```\n",
    "第1步：假设没有激活函数，h = W × 输入\n",
    "\n",
    "第2步：要让输出方差 = 输入方差\n",
    "       推出条件：输入维度 × 权重方差 = 1\n",
    "\n",
    "第3步：要让梯度方差也不变\n",
    "       推出条件：输出维度 × 权重方差 = 1\n",
    "\n",
    "第4步：两个条件不能同时满足（除非输入维度=输出维度）\n",
    "\n",
    "第5步：折中！取平均：\n",
    "       权重方差 = 2 / (输入维度 + 输出维度)\n",
    "       \n",
    "这就是Xavier初始化！\n",
    "```\n",
    "\n",
    "### 为什么激活函数要\"过原点且斜率为1\"\n",
    "\n",
    "```\n",
    "如果激活函数 f(x) = αx + β\n",
    "\n",
    "要让输出均值不变 → β必须=0（过原点）\n",
    "要让输出方差不变 → α必须=1（不放大不缩小）\n",
    "\n",
    "所以理想的激活函数在零点附近应该是 f(x) = x\n",
    "\n",
    "ReLU：x>0时 f(x)=x ✅\n",
    "tanh：零点附近 f(x)≈x ✅  \n",
    "Sigmoid：f(0)=0.5 ≠ 0 ❌（不过原点）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 不用管的\n",
    "\n",
    "| 内容 | 原因 |\n",
    "|------|------|\n",
    "| 方差推导的具体数学过程 | 知道结论就行 |\n",
    "| 均匀分布版Xavier的系数为什么是6 | 均匀分布方差公式，用到再查 |\n",
    "| 对角矩阵乘法的细节 | 理论推导用的 |\n",
    "| 泰勒展开的具体计算 | 数学工具，理解思想即可 |\n",
    "| 调整后的Sigmoid公式（4σ-2） | 实际几乎不用Sigmoid |\n",
    "\n",
    "---\n",
    "\n",
    "## ⭐ ⭐ ⭐ 写代码时的模板\n",
    "\n",
    "```python\n",
    "# 以前你写的（简单但不够好）：\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)  # 固定方差\n",
    "\n",
    "# 更好的写法（Xavier初始化）：\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)  # ⭐⭐⭐ 根据维度自动调整\n",
    "```\n",
    "\n",
    "```python\n",
    "# 激活函数选择：\n",
    "nn.ReLU()    # 默认首选，最常用\n",
    "nn.Tanh()    # 也可以\n",
    "# nn.Sigmoid()  # 尽量别用，除非你知道为什么要用\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "> **深网络容易梯度爆炸/消失 → 用Xavier初始化权重 + ReLU激活函数 → 让每层的值都保持在合理范围**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
