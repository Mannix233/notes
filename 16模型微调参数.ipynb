{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7023-d13a-4d4c-a57b-501d634f2062",
   "metadata": {},
   "source": [
    "# 参数管理：详细讲解\n",
    "\n",
    "---\n",
    "\n",
    "## 一、准备工作：先搭一个简单网络\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8),      # 第0层：全连接，4进8出\n",
    "    nn.ReLU(),             # 第1层：激活函数（没有参数）\n",
    "    nn.Linear(8, 1)        # 第2层：全连接，8进1出\n",
    ")\n",
    "\n",
    "X = torch.rand(2, 4)      # 2个样本，每个4个特征\n",
    "output = net(X)            # 输出形状：2×1\n",
    "```\n",
    "\n",
    "**网络结构：**\n",
    "\n",
    "```\n",
    "net[0] = Linear(4, 8)     ← 有参数（weight + bias）\n",
    "net[1] = ReLU()            ← 没有参数！\n",
    "net[2] = Linear(8, 1)     ← 有参数（weight + bias）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、怎么访问参数？\n",
    "\n",
    "### 2.1 net就像一个列表\n",
    "\n",
    "**Sequential就像一个列表，用编号取每一层：**\n",
    "\n",
    "```python\n",
    "net[0]    # 第0层：Linear(4, 8)\n",
    "net[1]    # 第1层：ReLU\n",
    "net[2]    # 第2层：Linear(8, 1)\n",
    "```\n",
    "\n",
    "**就像书架上的书：第0本、第1本、第2本。**\n",
    "\n",
    "### 2.2 每个Linear层里有什么？\n",
    "\n",
    "**每个全连接层里面有两样东西：**\n",
    "\n",
    "```\n",
    "weight（权重）：一个矩阵，用来做计算的\n",
    "bias（偏置）：一个向量，加上去的偏移\n",
    "```\n",
    "\n",
    "**怎么看？**\n",
    "\n",
    "```python\n",
    "net[2].state_dict()\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "OrderedDict([\n",
    "    ('weight', tensor([[-0.12, 0.34, ...]])),    # 权重矩阵\n",
    "    ('bias', tensor([0.05]))      # 偏置\n",
    "])\n",
    "```\n",
    "\n",
    "**翻译：**\n",
    "\n",
    "```\n",
    "net[2]           → 拿出第2层\n",
    ".state_dict()    → 把这一层的所有参数列出来\n",
    "```\n",
    "\n",
    "**state_dict就是\"状态字典\"，把参数的名字和值配对列出来。**\n",
    "\n",
    "### 2.3 直接访问某个具体参数\n",
    "\n",
    "```python\n",
    "net[2].weight        # 拿出第2层的权重（是一个Parameter对象）\n",
    "net[2].bias          # 拿出第2层的偏置\n",
    "\n",
    "net[2].weight.data   # 拿出权重的具体数值\n",
    "net[2].bias.data     # 拿出偏置的具体数值\n",
    "\n",
    "net[2].weight.grad   # 拿出权重的梯度（还没训练就是None）\n",
    "```\n",
    "\n",
    "**为什么有 .data 和 .grad？**\n",
    "\n",
    "```\n",
    "一个参数(Parameter)包含两样东西：\n",
    "├── .data  → 参数本身的值（比如权重是多少）\n",
    "└── .grad  → 这个参数的梯度（训练时反向传播算出来的）\n",
    "\n",
    "还没训练的时候，.grad = None（因为还没算过梯度）\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "一个学生(Parameter)有两个属性：\n",
    "├── .data  → 他现在的成绩（比如85分）\n",
    "└── .grad  → 他需要提高多少（比如+5分）\n",
    "\n",
    "还没考试的时候，.grad = None（不知道该提高多少）\n",
    "```\n",
    "\n",
    "### 2.4 一次性看所有层的参数\n",
    "\n",
    "```python\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "0.weight    torch.Size([8, 4])     # 第0层权重：8×4的矩阵\n",
    "0.bias      torch.Size([8])        # 第0层偏置：8个数\n",
    "2.weight    torch.Size([1, 8])     # 第2层权重：1×8的矩阵\n",
    "2.bias      torch.Size([1])        # 第2层偏置：1个数\n",
    "```\n",
    "\n",
    "**注意：第1层（ReLU）没有出现，因为ReLU没有参数！**\n",
    "\n",
    "**名字的规则：**\n",
    "\n",
    "```\n",
    "\"0.weight\"  → 第0层的权重\n",
    "\"0.bias\"    → 第0层的偏置\n",
    "\"2.weight\"  → 第2层的权重\n",
    "\"2.bias\"    → 第2层的偏置\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、怎么初始化参数？（重要！）\n",
    "\n",
    "### 3.1 为什么要初始化？\n",
    "\n",
    "**回忆Xavier初始化那一节：**\n",
    "\n",
    "```\n",
    "权重太大 → 信号爆炸\n",
    "权重太小 → 信号消失\n",
    "权重刚好 → 训练正常\n",
    "```\n",
    "\n",
    "**PyTorch有默认的初始化，但有时候你想自己控制。**\n",
    "\n",
    "### 3.2 最常用的初始化方法\n",
    "\n",
    "```python\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:                    # 如果是全连接层\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)  # 权重：正态分布\n",
    "        nn.init.zeros_(m.bias)                         # 偏置：全设为0\n",
    "\n",
    "net.apply(init_normal)    # 对net里的每一层都执行这个函数\n",
    "```\n",
    "\n",
    "### 逐行详细解释\n",
    "\n",
    "**第1行：定义一个函数**\n",
    "\n",
    "```python\n",
    "def init_normal(m):\n",
    "```\n",
    "\n",
    "```\n",
    "定义了一个函数叫 init_normal\n",
    "m 是传进来的\"一个层\"\n",
    "（apply会自动把每一层依次传进来）\n",
    "```\n",
    "\n",
    "**第2行：判断这个层是不是Linear**\n",
    "\n",
    "```python\n",
    "if type(m) == nn.Linear:\n",
    "```\n",
    "\n",
    "```\n",
    "为什么要判断？\n",
    "因为网络里有各种层：Linear、ReLU、BatchNorm...\n",
    "ReLU没有参数，你不能对它初始化\n",
    "所以要先看看：这个层是Linear吗？\n",
    "    是 → 执行初始化\n",
    "    不是 → 跳过，什么都不做\n",
    "```\n",
    "\n",
    "**第3行：初始化权重**\n",
    "\n",
    "```python\n",
    "nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "```\n",
    "\n",
    "```\n",
    "nn.init          → PyTorch提供的初始化工具包\n",
    ".normal_         → 用正态分布填充（下划线表示\"直接替换，不返回新值\"）\n",
    "m.weight         → 这一层的权重\n",
    "mean=0           → 均值为0\n",
    "std=0.01         → 标准差为0.01（很小的随机数）\n",
    "```\n",
    "\n",
    "**第4行：初始化偏置**\n",
    "\n",
    "```python\n",
    "nn.init.zeros_(m.bias)\n",
    "```\n",
    "\n",
    "```\n",
    "把偏置全部设为0\n",
    "这是最常见的做法\n",
    "```\n",
    "\n",
    "**第5行：apply**\n",
    "\n",
    "```python\n",
    "net.apply(init_normal)\n",
    "```\n",
    "\n",
    "```\n",
    "apply的意思：对net里面的每一层，依次执行init_normal函数\n",
    "\n",
    "它会这样做：\n",
    "    init_normal(net[0])    → net[0]是Linear → 执行初始化 ✓\n",
    "    init_normal(net[1])    → net[1]是ReLU   → 不是Linear → 跳过\n",
    "    init_normal(net[2])    → net[2]是Linear → 执行初始化 ✓\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "apply就像一个检查员，挨个房间检查：\n",
    "    第0个房间：是仓库（Linear）→ 整理一下 ✓\n",
    "    第1个房间：是厕所（ReLU） → 不用整理，跳过\n",
    "    第2个房间：是仓库（Linear）→ 整理一下 ✓\n",
    "```\n",
    "\n",
    "### 3.3 对不同层用不同的初始化\n",
    "\n",
    "```python\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)    # Xavier初始化\n",
    "\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)       # 全部设为1（别这么干！纯演示）\n",
    "\n",
    "net[0].apply(init_xavier)      # 第0层用Xavier\n",
    "net[2].apply(init_constant)    # 第2层用常数\n",
    "```\n",
    "\n",
    "```\n",
    "因为每一层本身也是Module\n",
    "所以可以单独对某一层调用apply\n",
    "不用整个网络一起改\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、怎么冻结参数？（微调必用！）\n",
    "\n",
    "### 4.1 什么叫冻结？\n",
    "\n",
    "```\n",
    "正常训练：所有参数都会被更新\n",
    "冻结某层：这一层的参数不动，只训练其他层\n",
    "```\n",
    "\n",
    "**什么时候用？**\n",
    "\n",
    "```\n",
    "微调(Fine-tuning)的时候！\n",
    "\n",
    "比如你用了一个别人训练好的大模型\n",
    "你只想改最后一层，前面的层保持不变\n",
    "→ 冻结前面的层，只训练最后一层\n",
    "```\n",
    "\n",
    "### 4.2 怎么冻结？\n",
    "\n",
    "**核心：设置 requires_grad = False**\n",
    "\n",
    "```python\n",
    "# 冻结第0层的所有参数\n",
    "for param in net[0].parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "**翻译：**\n",
    "\n",
    "```\n",
    "net[0].parameters()      → 拿出第0层的所有参数\n",
    "param.requires_grad      → 这个参数需要计算梯度吗？\n",
    "= False                  → 不需要！不要动它！\n",
    "```\n",
    "\n",
    "**设成False之后：**\n",
    "\n",
    "```\n",
    "训练的时候：\n",
    "    net[0]的权重和偏置 → 不会被更新（冻住了）❄️\n",
    "    net[2]的权重和偏置 → 正常更新 ✓\n",
    "```\n",
    "\n",
    "### 4.3 微调的完整例子\n",
    "\n",
    "```python\n",
    "# 假设net是别人训练好的模型\n",
    "\n",
    "# 第1步：冻结所有层\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 第2步：只解冻最后一层\n",
    "for param in net[2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 现在训练的话，只有最后一层会更新\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "一栋楼有3层\n",
    "你搬进来，发现1楼和2楼装修得很好，不想动\n",
    "只想重新装修3楼\n",
    "\n",
    "第1步：把所有楼层都锁上（全部冻结）\n",
    "第2步：只打开3楼的门（解冻最后一层）\n",
    "第3步：开始装修（训练）→ 只有3楼会变\n",
    "```\n",
    "\n",
    "### 4.4 检查哪些参数被冻结了\n",
    "\n",
    "```python\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "0.weight    False     ← 冻住了\n",
    "0.bias      False     ← 冻住了\n",
    "2.weight    True      ← 可以训练\n",
    "2.bias      True      ← 可以训练\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结\n",
    "\n",
    "### 必须记住\n",
    "\n",
    "```python\n",
    "# 1. 访问参数\n",
    "net[2].weight.data        # 看某一层的权重值\n",
    "net[2].bias.data          # 看某一层的偏置值\n",
    "\n",
    "# 2. 初始化参数\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "\n",
    "# 3. 冻结参数（微调必用！）\n",
    "for param in net[0].parameters():\n",
    "    param.requires_grad = False    # 这一层不训练了\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "| 操作 | 代码 | 什么时候用 |\n",
    "|:---|:---|:---|\n",
    "| 看参数 | `net[2].weight.data` | 调试、检查 |\n",
    "| 初始化 | `nn.init.normal_()` + `apply` | 训练开始前 |\n",
    "| 冻结 | `requires_grad = False` | **微调时！** |\n",
    "\n",
    "### 一句话总结\n",
    "\n",
    "> 参数管理 = **看参数**（访问）+ **改参数**（初始化）+ **锁参数**（冻结）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba757a9-d167-4408-b800-896274cf5833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
