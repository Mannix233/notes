{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7023-d13a-4d4c-a57b-501d634f2062",
   "metadata": {},
   "source": [
    "# 参数管理：详细讲解\n",
    "\n",
    "---\n",
    "\n",
    "## 一、准备工作：先搭一个简单网络\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8),      # 第0层：全连接，4进8出\n",
    "    nn.ReLU(),             # 第1层：激活函数（没有参数）\n",
    "    nn.Linear(8, 1)        # 第2层：全连接，8进1出\n",
    ")\n",
    "\n",
    "X = torch.rand(2, 4)      # 2个样本，每个4个特征\n",
    "output = net(X)            # 输出形状：2×1\n",
    "```\n",
    "\n",
    "**网络结构：**\n",
    "\n",
    "```\n",
    "net[0] = Linear(4, 8)     ← 有参数（weight + bias）\n",
    "net[1] = ReLU()            ← 没有参数！\n",
    "net[2] = Linear(8, 1)     ← 有参数（weight + bias）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、怎么访问参数？\n",
    "\n",
    "### 2.1 net就像一个列表\n",
    "\n",
    "**Sequential就像一个列表，用编号取每一层：**\n",
    "\n",
    "```python\n",
    "net[0]    # 第0层：Linear(4, 8)\n",
    "net[1]    # 第1层：ReLU\n",
    "net[2]    # 第2层：Linear(8, 1)\n",
    "```\n",
    "\n",
    "**就像书架上的书：第0本、第1本、第2本。**\n",
    "\n",
    "### 2.2 每个Linear层里有什么？\n",
    "\n",
    "**每个全连接层里面有两样东西：**\n",
    "\n",
    "```\n",
    "weight（权重）：一个矩阵，用来做计算的\n",
    "bias（偏置）：一个向量，加上去的偏移\n",
    "```\n",
    "\n",
    "**怎么看？**\n",
    "\n",
    "```python\n",
    "net[2].state_dict()\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "OrderedDict([\n",
    "    ('weight', tensor([[-0.12, 0.34, ...]])),    # 权重矩阵\n",
    "    ('bias', tensor([0.05]))      # 偏置\n",
    "])\n",
    "```\n",
    "\n",
    "**翻译：**\n",
    "\n",
    "```\n",
    "net[2]           → 拿出第2层\n",
    ".state_dict()    → 把这一层的所有参数列出来\n",
    "```\n",
    "\n",
    "**state_dict就是\"状态字典\"，把参数的名字和值配对列出来。**\n",
    "\n",
    "### 2.3 直接访问某个具体参数\n",
    "\n",
    "```python\n",
    "net[2].weight        # 拿出第2层的权重（是一个Parameter对象）\n",
    "net[2].bias          # 拿出第2层的偏置\n",
    "\n",
    "net[2].weight.data   # 拿出权重的具体数值\n",
    "net[2].bias.data     # 拿出偏置的具体数值\n",
    "\n",
    "net[2].weight.grad   # 拿出权重的梯度（还没训练就是None）\n",
    "```\n",
    "\n",
    "**为什么有 .data 和 .grad？**\n",
    "\n",
    "```\n",
    "一个参数(Parameter)包含两样东西：\n",
    "├── .data  → 参数本身的值（比如权重是多少）\n",
    "└── .grad  → 这个参数的梯度（训练时反向传播算出来的）\n",
    "\n",
    "还没训练的时候，.grad = None（因为还没算过梯度）\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "一个学生(Parameter)有两个属性：\n",
    "├── .data  → 他现在的成绩（比如85分）\n",
    "└── .grad  → 他需要提高多少（比如+5分）\n",
    "\n",
    "还没考试的时候，.grad = None（不知道该提高多少）\n",
    "```\n",
    "\n",
    "### 2.4 一次性看所有层的参数\n",
    "\n",
    "```python\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "0.weight    torch.Size([8, 4])     # 第0层权重：8×4的矩阵\n",
    "0.bias      torch.Size([8])        # 第0层偏置：8个数\n",
    "2.weight    torch.Size([1, 8])     # 第2层权重：1×8的矩阵\n",
    "2.bias      torch.Size([1])        # 第2层偏置：1个数\n",
    "```\n",
    "\n",
    "**注意：第1层（ReLU）没有出现，因为ReLU没有参数！**\n",
    "\n",
    "**名字的规则：**\n",
    "\n",
    "```\n",
    "\"0.weight\"  → 第0层的权重\n",
    "\"0.bias\"    → 第0层的偏置\n",
    "\"2.weight\"  → 第2层的权重\n",
    "\"2.bias\"    → 第2层的偏置\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、怎么初始化参数？（重要！）\n",
    "\n",
    "### 3.1 为什么要初始化？\n",
    "\n",
    "**回忆Xavier初始化那一节：**\n",
    "\n",
    "```\n",
    "权重太大 → 信号爆炸\n",
    "权重太小 → 信号消失\n",
    "权重刚好 → 训练正常\n",
    "```\n",
    "\n",
    "**PyTorch有默认的初始化，但有时候你想自己控制。**\n",
    "\n",
    "### 3.2 最常用的初始化方法\n",
    "\n",
    "```python\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:                    # 如果是全连接层\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)  # 权重：正态分布\n",
    "        nn.init.zeros_(m.bias)                         # 偏置：全设为0\n",
    "\n",
    "net.apply(init_normal)    # 对net里的每一层都执行这个函数\n",
    "```\n",
    "\n",
    "### 逐行详细解释\n",
    "\n",
    "**第1行：定义一个函数**\n",
    "\n",
    "```python\n",
    "def init_normal(m):\n",
    "```\n",
    "\n",
    "```\n",
    "定义了一个函数叫 init_normal\n",
    "m 是传进来的\"一个层\"\n",
    "（apply会自动把每一层依次传进来）\n",
    "```\n",
    "\n",
    "**第2行：判断这个层是不是Linear**\n",
    "\n",
    "```python\n",
    "if type(m) == nn.Linear:\n",
    "```\n",
    "\n",
    "```\n",
    "为什么要判断？\n",
    "因为网络里有各种层：Linear、ReLU、BatchNorm...\n",
    "ReLU没有参数，你不能对它初始化\n",
    "所以要先看看：这个层是Linear吗？\n",
    "    是 → 执行初始化\n",
    "    不是 → 跳过，什么都不做\n",
    "```\n",
    "\n",
    "**第3行：初始化权重**\n",
    "\n",
    "```python\n",
    "nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "```\n",
    "\n",
    "```\n",
    "nn.init          → PyTorch提供的初始化工具包\n",
    ".normal_         → 用正态分布填充（下划线表示\"直接替换，不返回新值\"）\n",
    "m.weight         → 这一层的权重\n",
    "mean=0           → 均值为0\n",
    "std=0.01         → 标准差为0.01（很小的随机数）\n",
    "```\n",
    "\n",
    "**第4行：初始化偏置**\n",
    "\n",
    "```python\n",
    "nn.init.zeros_(m.bias)\n",
    "```\n",
    "\n",
    "```\n",
    "把偏置全部设为0\n",
    "这是最常见的做法\n",
    "```\n",
    "\n",
    "**第5行：apply**\n",
    "\n",
    "```python\n",
    "net.apply(init_normal)\n",
    "```\n",
    "\n",
    "```\n",
    "apply的意思：对net里面的每一层，依次执行init_normal函数\n",
    "\n",
    "它会这样做：\n",
    "    init_normal(net[0])    → net[0]是Linear → 执行初始化 ✓\n",
    "    init_normal(net[1])    → net[1]是ReLU   → 不是Linear → 跳过\n",
    "    init_normal(net[2])    → net[2]是Linear → 执行初始化 ✓\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "apply就像一个检查员，挨个房间检查：\n",
    "    第0个房间：是仓库（Linear）→ 整理一下 ✓\n",
    "    第1个房间：是厕所（ReLU） → 不用整理，跳过\n",
    "    第2个房间：是仓库（Linear）→ 整理一下 ✓\n",
    "```\n",
    "---\n",
    "\n",
    "# 把初始化这件事彻底讲清楚\n",
    "\n",
    "## 核心困惑\n",
    "> \"type都是nn.Linear，那为什么上面的函数名字不一样？里面的操作也不一样？这到底在干什么？\"\n",
    "\n",
    "## 用最简单的比喻讲清楚\n",
    "\n",
    "### 想象你开了一家公司，有10个员工\n",
    "\n",
    "```\n",
    "员工分两种：\n",
    "    程序员（相当于 nn.Linear，有技能要设置）\n",
    "    保安（相当于 nn.ReLU，不需要设置什么）\n",
    "```\n",
    "\n",
    "### 你要给程序员分配工作方式\n",
    "\n",
    "**但是分配方式可以不同！**\n",
    "\n",
    "```\n",
    "方案A（init_normal）：让每个程序员随机分配任务\n",
    "方案B（init_xavier）：让每个程序员按Xavier方法分配任务\n",
    "方案C（init_constant）：让每个程序员都做一样的任务\n",
    "```\n",
    "\n",
    "**不管哪个方案，你都只找程序员，不找保安**\n",
    "\n",
    "```python\n",
    "if type(m) == nn.Linear:    # 你是程序员吗？\n",
    "                            # 是 → 给你分配工作\n",
    "                            # 不是（保安/ReLU）→ 跳过\n",
    "```\n",
    "\n",
    "**这就是为什么type都是nn.Linear，但函数名和操作不一样！**\n",
    "\n",
    "---\n",
    "\n",
    "## 拆开看每个函数\n",
    "\n",
    "### 函数1：init_normal\n",
    "\n",
    "```python\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "```\n",
    "\n",
    "**它做了什么？**\n",
    "\n",
    "```\n",
    "找到Linear层 → 权重用正态分布随机填（均值0，标准差0.01）\n",
    "              → 偏置全部设为0\n",
    "```\n",
    "\n",
    "### 函数2：init_xavier\n",
    "\n",
    "```python\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "```\n",
    "\n",
    "**它做了什么？**\n",
    "\n",
    "```\n",
    "找到Linear层 → 权重用Xavier方法填\n",
    "```\n",
    "\n",
    "### 函数3：init_constant\n",
    "\n",
    "```python\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "```\n",
    "\n",
    "**它做了什么？**\n",
    "\n",
    "```\n",
    "找到Linear层 → 权重全部设为1（纯演示，别这么干！）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 关键理解：这三个函数结构完全一样！\n",
    "\n",
    "```python\n",
    "def 函数名(m):                          # ← 名字随便起\n",
    "    if type(m) == nn.Linear:            # ← 固定的：只处理Linear层\n",
    "        nn.init.某种方法_(m.weight)      # ← 这里不同！填充方式不同！\n",
    "```\n",
    "\n",
    "**唯一的区别就是第三行：用什么方式填充权重**\n",
    "\n",
    "| 函数名 | 填充方式 | 效果 |\n",
    "|:---|:---|:---|\n",
    "| init_normal | `nn.init.normal_()` | 随机小数字 |\n",
    "| init_xavier | `nn.init.xavier_uniform_()` | Xavier方法的随机数 |\n",
    "| init_constant | `nn.init.constant_(, 1)` | 全部变成1 |\n",
    "\n",
    "---\n",
    "\n",
    "## 为什么要定义不同的初始化函数？\n",
    "\n",
    "### 因为不同情况需要不同的初始化！\n",
    "\n",
    "**回忆Xavier那一节：**\n",
    "\n",
    "```\n",
    "用tanh激活函数 → 适合Xavier初始化\n",
    "用ReLU激活函数 → 适合He初始化\n",
    "普通场景       → 正态分布初始化也行\n",
    "```\n",
    "\n",
    "**所以你可能想：**\n",
    "\n",
    "```\n",
    "第0层用Xavier初始化\n",
    "第2层用正态分布初始化\n",
    "```\n",
    "\n",
    "**怎么做？**\n",
    "\n",
    "```python\n",
    "net[0].apply(init_xavier)      # 只对第0层用Xavier\n",
    "net[2].apply(init_normal)      # 只对第2层用正态分布\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 用更生活化的比喻\n",
    "\n",
    "### 你有一个工具箱（nn.init）\n",
    "\n",
    "```\n",
    "工具箱里有很多工具：\n",
    "    nn.init.normal_()          → 随机撒种子\n",
    "    nn.init.xavier_uniform_()  → 按Xavier方法撒种子\n",
    "    nn.init.constant_(, 1)     → 全部放一样的种子\n",
    "    nn.init.zeros_()           → 什么都不放\n",
    "```\n",
    "\n",
    "### 你有一块地（网络），地里有不同的区域\n",
    "\n",
    "```\n",
    "第0块地 = Linear层（需要种东西）\n",
    "第1块地 = ReLU（是条路，不需要种）\n",
    "第2块地 = Linear层（需要种东西）\n",
    "```\n",
    "\n",
    "### 你写一个种地计划\n",
    "\n",
    "```python\n",
    "# 种地计划A：用随机撒种子的方式\n",
    "def plan_a(这块地):\n",
    "    if 这块地需要种东西:           # if type(m) == nn.Linear\n",
    "        随机撒种子(这块地)          # nn.init.normal_(m.weight)\n",
    "\n",
    "# 种地计划B：用Xavier方式\n",
    "def plan_b(这块地):\n",
    "    if 这块地需要种东西:           # if type(m) == nn.Linear\n",
    "        Xavier撒种子(这块地)       # nn.init.xavier_uniform_(m.weight)\n",
    "```\n",
    "\n",
    "### 然后你决定哪块地用哪个计划\n",
    "\n",
    "```python\n",
    "第0块地.apply(plan_b)      # 第0块地用Xavier\n",
    "第2块地.apply(plan_a)      # 第2块地用随机撒\n",
    "```\n",
    "\n",
    "**计划的结构一样（都是先判断再操作），但具体操作不同！**\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 为什么type都是nn.Linear？\n",
    "\n",
    "```\n",
    "因为只有Linear层有权重需要初始化\n",
    "ReLU没有权重，所以跳过\n",
    "这个判断是固定的，每个初始化函数都要写\n",
    "```\n",
    "\n",
    "### 为什么函数名和操作不一样？\n",
    "\n",
    "```\n",
    "因为\"怎么初始化\"有很多种方式\n",
    "就像\"怎么做饭\"有炒、煮、蒸\n",
    "不管哪种方式，你都是在厨房做（都判断nn.Linear）\n",
    "但具体动作不同（normal、xavier、constant）\n",
    "```\n",
    "\n",
    "### 你需要记住的\n",
    "\n",
    "```\n",
    "1. 初始化函数的固定模式：\n",
    "   def 函数名(m):\n",
    "       if type(m) == nn.Linear:\n",
    "           nn.init.某种方法_(m.weight)\n",
    "\n",
    "2. 常用的方法：\n",
    "   nn.init.normal_()           → 正态分布（通用）\n",
    "   nn.init.xavier_uniform_()   → Xavier（配合tanh）\n",
    "   nn.init.kaiming_normal_()   → He初始化（配合ReLU）\n",
    "   nn.init.zeros_()            → 全设为0（常用于bias）\n",
    "\n",
    "3. 怎么用：\n",
    "   net.apply(函数名)           → 对所有层执行\n",
    "   net[0].apply(函数名)        → 只对第0层执行\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 四、怎么冻结参数？（微调必用！）\n",
    "\n",
    "### 4.1 什么叫冻结？\n",
    "\n",
    "```\n",
    "正常训练：所有参数都会被更新\n",
    "冻结某层：这一层的参数不动，只训练其他层\n",
    "```\n",
    "\n",
    "**什么时候用？**\n",
    "\n",
    "```\n",
    "微调(Fine-tuning)的时候！\n",
    "\n",
    "比如你用了一个别人训练好的大模型\n",
    "你只想改最后一层，前面的层保持不变\n",
    "→ 冻结前面的层，只训练最后一层\n",
    "```\n",
    "\n",
    "### 4.2 怎么冻结？\n",
    "\n",
    "**核心：设置 requires_grad = False**\n",
    "\n",
    "```python\n",
    "# 冻结第0层的所有参数\n",
    "for param in net[0].parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "**翻译：**\n",
    "\n",
    "```\n",
    "net[0].parameters()      → 拿出第0层的所有参数\n",
    "param.requires_grad      → 这个参数需要计算梯度吗？\n",
    "= False                  → 不需要！不要动它！\n",
    "```\n",
    "\n",
    "**设成False之后：**\n",
    "\n",
    "```\n",
    "训练的时候：\n",
    "    net[0]的权重和偏置 → 不会被更新（冻住了）❄️\n",
    "    net[2]的权重和偏置 → 正常更新 ✓\n",
    "```\n",
    "\n",
    "### 4.3 微调的完整例子\n",
    "\n",
    "```python\n",
    "# 假设net是别人训练好的模型\n",
    "\n",
    "# 第1步：冻结所有层\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 第2步：只解冻最后一层\n",
    "for param in net[2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 现在训练的话，只有最后一层会更新\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "一栋楼有3层\n",
    "你搬进来，发现1楼和2楼装修得很好，不想动\n",
    "只想重新装修3楼\n",
    "\n",
    "第1步：把所有楼层都锁上（全部冻结）\n",
    "第2步：只打开3楼的门（解冻最后一层）\n",
    "第3步：开始装修（训练）→ 只有3楼会变\n",
    "```\n",
    "\n",
    "### 4.4 检查哪些参数被冻结了\n",
    "\n",
    "```python\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "```\n",
    "\n",
    "```\n",
    "输出：\n",
    "0.weight    False     ← 冻住了\n",
    "0.bias      False     ← 冻住了\n",
    "2.weight    True      ← 可以训练\n",
    "2.bias      True      ← 可以训练\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结\n",
    "\n",
    "### 必须记住\n",
    "\n",
    "```python\n",
    "# 1. 访问参数\n",
    "net[2].weight.data        # 看某一层的权重值\n",
    "net[2].bias.data          # 看某一层的偏置值\n",
    "\n",
    "# 2. 初始化参数\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "\n",
    "# 3. 冻结参数（微调必用！）\n",
    "for param in net[0].parameters():\n",
    "    param.requires_grad = False    # 这一层不训练了\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "| 操作 | 代码 | 什么时候用 |\n",
    "|:---|:---|:---|\n",
    "| 看参数 | `net[2].weight.data` | 调试、检查 |\n",
    "| 初始化 | `nn.init.normal_()` + `apply` | 训练开始前 |\n",
    "| 冻结 | `requires_grad = False` | **微调时！** |\n",
    "\n",
    "### 一句话总结\n",
    "\n",
    "> 参数管理 = **看参数**（访问）+ **改参数**（初始化）+ **锁参数**（冻结）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba757a9-d167-4408-b800-896274cf5833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
