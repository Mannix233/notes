{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1ffa6b-9f95-4a22-b582-6ab6e4be3fe2",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "## 一、为什么需要ResNet？\n",
    "\n",
    "### 更深的网络不一定更好\n",
    "\n",
    "```\n",
    "正常的想法：\n",
    "    10层网络 → 还行\n",
    "    20层网络 → 更好\n",
    "    50层网络 → 应该更更好？\n",
    "\n",
    "实际情况：\n",
    "    10层 → 还行\n",
    "    20层 → 更好\n",
    "    50层 → 反而变差了！\n",
    "\n",
    "不是过拟合！是网络太深，梯度传不到底层，底层根本学不动\n",
    "```\n",
    "\n",
    "### ResNet怎么解决？\n",
    "\n",
    "**加一条捷径，让输入可以跳过某些层直接传过去**\n",
    "\n",
    "```\n",
    "普通网络：\n",
    "    x → 卷积 → 卷积 → 输出F(x)\n",
    "\n",
    "ResNet：\n",
    "    x → 卷积 → 卷积 → 结果F(x)\n",
    "    x ─────────────────→ +       ← 把x加回来！\n",
    "                          ↓\n",
    "                    输出 = F(x) + x\n",
    "```\n",
    "\n",
    "### 为什么 +x 有用？\n",
    "\n",
    "```\n",
    "中间的层学到了东西：\n",
    "    输出 = 有用的东西 + x → 比x更好 ✓\n",
    "\n",
    "中间的层什么都没学到：\n",
    "    输出 = 0 + x = x → 至少不会更差 ✓\n",
    "\n",
    "所以加层只会变好或不变，永远不会变差\n",
    "这就是为什么ResNet可以做到100层甚至1000层\n",
    "```\n",
    "---\n",
    "\n",
    "## 二、残差连接长什么样？\n",
    "\n",
    "```\n",
    "输入x\n",
    "  │\n",
    "  ├─────────────────────────┐\n",
    "  │                         │ 这条线就是\"捷径\"\n",
    "  ↓                         │\n",
    "Conv → BN → ReLU → Conv → BN   │\n",
    "  │                         │\n",
    "  ↓                         │\n",
    "  + ←───────────────────────┘  把x加回来\n",
    "  ↓\n",
    " ReLU\n",
    "  ↓\n",
    " 输出\n",
    "```\n",
    "\n",
    "**整个ResNet就是把很多这样的块堆在一起**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、ResNet的整体结构\n",
    "\n",
    "```\n",
    "和LeNet套路一样：\n",
    "\n",
    "LeNet：  Conv → Pool → Conv → Pool → Flatten → FC\n",
    "ResNet： Conv → 残差块×很多 → 全局Pool → Flatten → FC\n",
    "\n",
    "区别只是中间换成了残差块\n",
    "```\n",
    "\n",
    "### 数据怎么变的\n",
    "\n",
    "```\n",
    "通道越来越多：  64 → 128 → 256 → 512\n",
    "空间越来越小：  56 → 28  → 14  → 7\n",
    "最后：全局池化 → Flatten → Linear\n",
    "\n",
    "跟LeNet一模一样的思路！\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、不同版本\n",
    "\n",
    "```\n",
    "ResNet18：18层，最快\n",
    "ResNet34：34层，常用\n",
    "ResNet50：50层，更强但更慢\n",
    "\n",
    "数字 = 层数，越大越强越慢\n",
    "实际中最常用：ResNet18 或 ResNet34\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# 从最基础开始：搞懂训练到底在做什么\n",
    "\n",
    "\n",
    "## 第一个问题：训练的完整流程是什么？\n",
    "\n",
    "### 不管什么网络，训练永远是这4步\n",
    "\n",
    "```\n",
    "第1步：定义网络（告诉电脑你的模型长什么样）\n",
    "第2步：准备数据（告诉电脑你的训练数据在哪）\n",
    "第3步：训练（让电脑反复学习）\n",
    "第4步：评估（看看学得怎么样）\n",
    "```\n",
    "\n",
    "**从LeNet到ResNet到任何网络，这4步永远不变！**\n",
    "\n",
    "---\n",
    "\n",
    "## 第二个问题：到底写不写网络？\n",
    "\n",
    "### 两种方式，结果完全一样\n",
    "\n",
    "**方式A：自己一层一层写（LeNet就是这么干的）**\n",
    "\n",
    "```python\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(400, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10),\n",
    ")\n",
    "```\n",
    "**方式B：直接用别人写好的结构（ResNet这么干）**\n",
    "\n",
    "```python\n",
    "net = models.resnet18()\n",
    "net.fc = nn.Linear(512, 10)\n",
    "```\n",
    "\n",
    "```\n",
    "PyTorch已经帮你把ResNet的结构写好了\n",
    "你直接拿来用就行\n",
    "就像买一套精装房，只需要换个门锁（改最后一层）\n",
    "```\n",
    "\n",
    "### 为什么ResNet不自己写？\n",
    "\n",
    "```\n",
    "LeNet：就5-6层，自己写很简单\n",
    "ResNet18：18层，自己写很麻烦\n",
    "ResNet50：50层，自己写要疯\n",
    "\n",
    "所以PyTorch帮你写好了，直接调用就行\n",
    "```\n",
    "\n",
    "### 但是！两种方式后面的训练代码完全一样！\n",
    "\n",
    "```python\n",
    "# 不管你的net是怎么来的\n",
    "# 后面这些代码一个字都不用变：\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for X, y in train_loader:\n",
    "        loss = loss_fn(net(X), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 第三个问题：为什么要改最后一层？\n",
    "\n",
    "### 用买房子来理解\n",
    "\n",
    "```\n",
    "PyTorch自带的ResNet18 = 一套精装房\n",
    "\n",
    "这套房子原来是给\"1000个人住的\"（ImageNet有1000类）\n",
    "你只需要\"10个人住\"（你的数据只有10类）\n",
    "\n",
    "所以你要把最后一个房间改小一点\n",
    "从1000人的改成10人的\n",
    "\n",
    "net = models.resnet18()          # 买了一套1000人的房子\n",
    "net.fc = nn.Linear(512, 10)      # 把最后一个房间改成10人的\n",
    "```\n",
    "\n",
    "### 改之前 vs 改之后\n",
    "\n",
    "```\n",
    "改之前：net的最后一层输出1000个数字（对应1000类）\n",
    "改之后：net的最后一层输出10个数字（对应你的10类）\n",
    "\n",
    "只改了最后一层！前面所有层都不用动！\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 第四个问题：net.train() 和 net.eval() 是什么？\n",
    "\n",
    "```python\n",
    "net.train()    # 告诉网络：现在是训练时间\n",
    "net.eval()     # 告诉网络：现在是考试时间\n",
    "```\n",
    "\n",
    "### 为什么要区分？\n",
    "\n",
    "```\n",
    "训练时（net.train()）：\n",
    "    BN层用当前batch的均值方差\n",
    "    Dropout随机丢弃一些神经元\n",
    "    → 正常学习\n",
    "\n",
    "评估时（net.eval()）：\n",
    "    BN层用全局的均值方差\n",
    "    Dropout关闭（不丢弃）\n",
    "    → 稳定地给出预测结果\n",
    "\n",
    "如果评估时忘了写net.eval()：\n",
    "    BN还在用batch的统计量 → 结果不稳定\n",
    "    Dropout还在丢弃 → 结果变差\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 第五个问题：预训练和冻结是怎么回事？\n",
    "\n",
    "### 现在不需要管！\n",
    "\n",
    "```\n",
    "这一节讲的ResNet = 从头训练\n",
    "    net = models.resnet18()          ← 随机权重，从零开始学\n",
    "    \n",
    "后面\"微调\"那一节 = 加载预训练权重\n",
    "    net = models.resnet18(pretrained=True)  ← 别人训练好的权重\n",
    "    冻结前面的层\n",
    "    只训练最后一层\n",
    "\n",
    "这是两件完全不同的事！\n",
    "现在只管\"从头训练\"就行！\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 把整个训练流程用做菜来比喻\n",
    "\n",
    "```\n",
    "第1步：选菜谱（定义网络）\n",
    "    LeNet：简单家常菜的菜谱（自己写）\n",
    "    ResNet：米其林大厨的菜谱（PyTorch写好的，直接用）\n",
    "\n",
    "第2步：买菜（准备数据）\n",
    "    train_loader = 买好的食材\n",
    "\n",
    "第3步：做菜（训练）\n",
    "    for epoch：做很多遍\n",
    "        net(X)：按菜谱做一遍\n",
    "        loss：尝一口，看看好不好吃\n",
    "        backward：想想哪里要改\n",
    "        step：调整一下\n",
    "\n",
    "第4步：请人品尝（评估）\n",
    "    net.eval()：告诉厨师现在是正式上菜\n",
    "    看看准确率多少\n",
    "```\n",
    "\n",
    "**不管用什么菜谱（LeNet还是ResNet），做菜的步骤都一样！**\n",
    "\n",
    "---\n",
    "\n",
    "## 最终版代码模板（标注每一行在干什么）\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# ====== 第1步：选菜谱 ======\n",
    "# 方式A（简单网络，自己写）：\n",
    "# net = nn.Sequential(nn.Conv2d(...), nn.ReLU(), ...)\n",
    "\n",
    "# 方式B（复杂网络，用现成的）：\n",
    "net = models.resnet18()              # 拿到ResNet的结构\n",
    "net.fc = nn.Linear(512, 10)          # 改最后一层（10个类别）\n",
    "\n",
    "\n",
    "# ====== 第2步：准备工具 ======\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)                       # 模型搬到GPU\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)  # 优化器\n",
    "loss_fn = nn.CrossEntropyLoss()      # 损失函数\n",
    "\n",
    "\n",
    "# ====== 第3步：训练 ======\n",
    "for epoch in range(10):              # 学10遍\n",
    "    net.train()                      # 切换到训练模式\n",
    "    for X, y in train_loader:        # 每次拿一批数据\n",
    "        X, y = X.to(device), y.to(device)   # 数据搬到GPU\n",
    "        y_hat = net(X)               # 前向传播：预测\n",
    "        loss = loss_fn(y_hat, y)     # 算损失：预测vs真实\n",
    "        optimizer.zero_grad()        # 清零梯度\n",
    "        loss.backward()              # 反向传播：算梯度\n",
    "        optimizer.step()             # 更新权重\n",
    "\n",
    "\n",
    "# ====== 第4步：评估 ======\n",
    "    net.eval()                       # 切换到评估模式\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():            # 评估时不需要算梯度\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.len()\n",
    "    print(f'准确率: {correct/total}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "```\n",
    "1. 定义网络有两种方式：自己写 或 用现成的\n",
    "   后面的训练代码完全一样\n",
    "\n",
    "2. models.resnet18() = PyTorch帮你写好的ResNet结构\n",
    "   不是预训练！就是个空壳子，权重是随机的\n",
    "\n",
    "3. 改最后一层 = 把输出改成你的类别数\n",
    "   因为默认是1000类，你可能只要10类\n",
    "\n",
    "4. net.train() = 训练模式\n",
    "   net.eval()  = 评估模式\n",
    "   每次训练前写train()，评估前写eval()\n",
    "\n",
    "5. 预训练、冻结、微调 = 后面的课才讲\n",
    "   现在不用管！\n",
    "```\n",
    "---\n",
    "\n",
    "## 七、什么时候改最后一层的512？\n",
    "\n",
    "```python\n",
    "net.fc = nn.Linear(512, 10)\n",
    "#                  ^^^\n",
    "#                  这个512哪来的？\n",
    "```\n",
    "\n",
    "```\n",
    "ResNet18/34：最后一层输入是512   → nn.Linear(512, 类别数)\n",
    "ResNet50/101/152：最后一层输入是2048 → nn.Linear(2048, 类别数)\n",
    "\n",
    "不确定的话：\n",
    "    print(net.fc)\n",
    "    # 会告诉你 Linear(in_features=???, out_features=1000)\n",
    "    # 那个???就是你要用的数字\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 概念\n",
    "\n",
    "```\n",
    "ResNet = 每隔几层加一条捷径（输出 = F(x) + x）\n",
    "好处：网络再深也能训练\n",
    "是现代深度学习的标配\n",
    "```\n",
    "\n",
    "### 代码模板\n",
    "\n",
    "```python\n",
    "# 定义网络\n",
    "net = models.resnet18()          # 或resnet34, resnet50\n",
    "net.fc = nn.Linear(512, 类别数)   # 改最后一层\n",
    "\n",
    "# 训练：和之前完全一样\n",
    "net.to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# for循环训练...\n",
    "```\n",
    "\n",
    "### 要记住的\n",
    "\n",
    "```\n",
    "1. ResNet的核心 = Y += X（残差连接）\n",
    "2. 实际中直接用 models.resnet18()，不用自己写\n",
    "3. 记得改最后一层 net.fc = nn.Linear(512, 你的类别数)\n",
    "4. 训练代码和LeNet完全一样，一个字不用改\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27ccc3-ee63-43ff-967c-471e3ee78fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
