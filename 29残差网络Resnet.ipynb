{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1ffa6b-9f95-4a22-b582-6ab6e4be3fe2",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "## 一、为什么需要ResNet？\n",
    "\n",
    "### 更深的网络不一定更好\n",
    "\n",
    "```\n",
    "正常的想法：\n",
    "    10层网络 → 还行\n",
    "    20层网络 → 更好\n",
    "    50层网络 → 应该更更好？\n",
    "\n",
    "实际情况：\n",
    "    10层 → 还行\n",
    "    20层 → 更好\n",
    "    50层 → 反而变差了！\n",
    "\n",
    "不是过拟合！是网络太深，梯度传不到底层，底层根本学不动\n",
    "```\n",
    "\n",
    "### ResNet怎么解决？\n",
    "\n",
    "**加一条捷径，让输入可以跳过某些层直接传过去**\n",
    "\n",
    "```\n",
    "普通网络：\n",
    "    x → 卷积 → 卷积 → 输出F(x)\n",
    "\n",
    "ResNet：\n",
    "    x → 卷积 → 卷积 → 结果F(x)\n",
    "    x ─────────────────→ +       ← 把x加回来！\n",
    "                          ↓\n",
    "                    输出 = F(x) + x\n",
    "```\n",
    "\n",
    "### 为什么 +x 有用？\n",
    "\n",
    "```\n",
    "中间的层学到了东西：\n",
    "    输出 = 有用的东西 + x → 比x更好 ✓\n",
    "\n",
    "中间的层什么都没学到：\n",
    "    输出 = 0 + x = x → 至少不会更差 ✓\n",
    "\n",
    "所以加层只会变好或不变，永远不会变差\n",
    "这就是为什么ResNet可以做到100层甚至1000层\n",
    "```\n",
    "---\n",
    "\n",
    "## 二、残差连接长什么样？\n",
    "\n",
    "```\n",
    "输入x\n",
    "  │\n",
    "  ├─────────────────────────┐\n",
    "  │                         │ 这条线就是\"捷径\"\n",
    "  ↓                         │\n",
    "Conv → BN → ReLU → Conv → BN   │\n",
    "  │                         │\n",
    "  ↓                         │\n",
    "  + ←───────────────────────┘  把x加回来\n",
    "  ↓\n",
    " ReLU\n",
    "  ↓\n",
    " 输出\n",
    "```\n",
    "\n",
    "**整个ResNet就是把很多这样的块堆在一起**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、ResNet的整体结构\n",
    "\n",
    "```\n",
    "和LeNet套路一样：\n",
    "\n",
    "LeNet：  Conv → Pool → Conv → Pool → Flatten → FC\n",
    "ResNet： Conv → 残差块×很多 → 全局Pool → Flatten → FC\n",
    "\n",
    "区别只是中间换成了残差块\n",
    "```\n",
    "\n",
    "### 数据怎么变的\n",
    "\n",
    "```\n",
    "通道越来越多：  64 → 128 → 256 → 512\n",
    "空间越来越小：  56 → 28  → 14  → 7\n",
    "最后：全局池化 → Flatten → Linear\n",
    "\n",
    "跟LeNet一模一样的思路！\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、不同版本\n",
    "\n",
    "```\n",
    "ResNet18：18层，最快\n",
    "ResNet34：34层，常用\n",
    "ResNet50：50层，更强但更慢\n",
    "\n",
    "数字 = 层数，越大越强越慢\n",
    "实际中最常用：ResNet18 或 ResNet34\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 五、代码实现\n",
    "\n",
    "### 直接用PyTorch自带的（实际中用）\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "net = models.resnet18()    # 拿到ResNet18的结构（随机权重）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、怎么训练？和LeNet完全一样！\n",
    "\n",
    "### 唯一要改的：最后一层\n",
    "\n",
    "```python\n",
    "# ResNet18默认输出1000类（ImageNet的类别数）\n",
    "# 你的任务可能是10类，需要改一下\n",
    "\n",
    "net = models.resnet18()\n",
    "net.fc = nn.Linear(512, 10)    # 改成你的类别数\n",
    "```\n",
    "\n",
    "**为什么要改？**\n",
    "\n",
    "```\n",
    "PyTorch自带的ResNet是按ImageNet设计的（1000类）\n",
    "你的数据集可能只有10类\n",
    "最后一层的输出大小必须等于你的类别数\n",
    "所以要手动改一下\n",
    "```\n",
    "\n",
    "### 完整训练代码\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# ========== 第1步：定义网络 ==========\n",
    "net = models.resnet18()   # 拿到ResNet结构\n",
    "net.fc = nn.Linear(512, 10) # 改最后一层为10类\n",
    "\n",
    "# ========== 第2步：搬到GPU ==========\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "\n",
    "# ========== 第3步：定义优化器和损失 ==========\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# ========== 第4步：训练（和LeNet一模一样！） ==========\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat = net(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 评估\n",
    "    net.eval()\n",
    "    # ...\n",
    "```\n",
    "\n",
    "### 对比LeNet的训练代码\n",
    "\n",
    "```python\n",
    "# LeNet\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.Sigmoid(), nn.AvgPool2d(2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.Sigmoid(), nn.AvgPool2d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16*5*5, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10),\n",
    ")\n",
    "\n",
    "# ResNet\n",
    "net = models.resnet18()\n",
    "net.fc = nn.Linear(512, 10)\n",
    "\n",
    "# 后面的训练代码：完全一样！\n",
    "# optimizer, loss_fn, for循环, backward, step...全一样\n",
    "```\n",
    "\n",
    "**区别只在定义网络那两行，训练代码一个字都不用改！**\n",
    "\n",
    "---\n",
    "\n",
    "## 七、什么时候改最后一层的512？\n",
    "\n",
    "```python\n",
    "net.fc = nn.Linear(512, 10)\n",
    "#                  ^^^\n",
    "#                  这个512哪来的？\n",
    "```\n",
    "\n",
    "```\n",
    "ResNet18/34：最后一层输入是512   → nn.Linear(512, 类别数)\n",
    "ResNet50/101/152：最后一层输入是2048 → nn.Linear(2048, 类别数)\n",
    "\n",
    "不确定的话：\n",
    "    print(net.fc)\n",
    "    # 会告诉你 Linear(in_features=???, out_features=1000)\n",
    "    # 那个???就是你要用的数字\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 八、总结\n",
    "\n",
    "### 概念\n",
    "\n",
    "```\n",
    "ResNet = 每隔几层加一条捷径（输出 = F(x) + x）\n",
    "好处：网络再深也能训练\n",
    "是现代深度学习的标配\n",
    "```\n",
    "\n",
    "### 代码模板\n",
    "\n",
    "```python\n",
    "# 定义网络\n",
    "net = models.resnet18()          # 或resnet34, resnet50\n",
    "net.fc = nn.Linear(512, 类别数)   # 改最后一层\n",
    "\n",
    "# 训练：和之前完全一样\n",
    "net.to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# for循环训练...\n",
    "```\n",
    "\n",
    "### 要记住的\n",
    "\n",
    "```\n",
    "1. ResNet的核心 = Y += X（残差连接）\n",
    "2. 实际中直接用 models.resnet18()，不用自己写\n",
    "3. 记得改最后一层 net.fc = nn.Linear(512, 你的类别数)\n",
    "4. 训练代码和LeNet完全一样，一个字不用改\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27ccc3-ee63-43ff-967c-471e3ee78fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
