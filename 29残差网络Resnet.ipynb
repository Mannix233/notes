{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1ffa6b-9f95-4a22-b582-6ab6e4be3fe2",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    " **理解残差连接的思想\n",
    " 会调用PyTorch自带的ResNet\n",
    " 知道怎么改最后一层**\n",
    " \n",
    "## 一、ResNet到底在干什么？\n",
    "\n",
    "### 普通网络的问题\n",
    "\n",
    "```\n",
    "想象你在爬100层楼梯\n",
    "\n",
    "普通网络 = 只有楼梯\n",
    "    爬到第50层你就累死了\n",
    "    信息（梯度）传到底层已经没力气了\n",
    "    底层学不动 → 整个网络学偏\n",
    "\n",
    "实际现象：\n",
    "    20层网络比10层好\n",
    "    50层网络反而比20层差了！\n",
    "    不是过拟合，是根本训练不动\n",
    "```\n",
    "\n",
    "### ResNet的解决方法\n",
    "\n",
    "```\n",
    "ResNet = 楼梯 + 电梯\n",
    "\n",
    "每隔几层就装一部电梯\n",
    "就算楼梯走不通，电梯也能直达\n",
    "\n",
    "结果：100层甚至1000层都能训练！\n",
    "```\n",
    "\n",
    "### 翻译成代码语言\n",
    "\n",
    "```\n",
    "普通网络：\n",
    "    输出 = F(x)\n",
    "    x经过几个层变成F(x)，完了\n",
    "\n",
    "ResNet：\n",
    "    输出 = F(x) + x      ← 就多了一个 +x ！\n",
    "    \n",
    "    那个 +x 就是\"电梯\"\n",
    "    不管F(x)学没学到东西，x都能直接传过去\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、残差连接画成图\n",
    "\n",
    "```\n",
    "最简单的理解：\n",
    "\n",
    "输入x ──→ 卷积 → BN → ReLU → 卷积 → BN ──→ 相加 → ReLU → 输出\n",
    "  │                                              ↑\n",
    "  └──────────────────────────────────────────────┘\n",
    "                    这条线就是残差连接\n",
    "                    x直接跳过去加到结果上\n",
    "```\n",
    "\n",
    "**就是加了一条线，让输入可以\"抄近路\"到输出！**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、为什么 +x 这么厉害？\n",
    "\n",
    "```\n",
    "情况1：中间的层学到了有用的东西\n",
    "    输出 = 有用的东西 + x = 比x更好 ✓\n",
    "\n",
    "情况2：中间的层什么都没学到（输出≈0）\n",
    "    输出 = 0 + x = x\n",
    "    至少不会比没加这些层更差 ✓\n",
    "\n",
    "所以：\n",
    "    加层只会变好或不变\n",
    "    永远不会变差！\n",
    "    这就是为什么ResNet可以做到100+层\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、ResNet的整体结构（知道大概）\n",
    "\n",
    "```\n",
    "图片进来\n",
    "    ↓\n",
    "一个大卷积快速缩小图片\n",
    "    ↓\n",
    "一堆残差块（通道越来越多，图片越来越小）\n",
    "    ↓\n",
    "全局平均池化（不管图片多大，变成1×1）\n",
    "    ↓\n",
    "一个全连接层输出分类结果\n",
    "```\n",
    "\n",
    "**和LeNet的套路完全一样，只是中间换成了残差块！**\n",
    "\n",
    "```\n",
    "LeNet：  Conv → Pool → Conv → Pool → FC\n",
    "ResNet：Conv → 残差块×N → 全局Pool → FC\n",
    "\n",
    "核心区别就是中间用了残差块（带+x的那种）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 五、不同版本的ResNet\n",
    "\n",
    "```\n",
    "ResNet18：18层，最快，精度一般\n",
    "ResNet34：34层，常用，速度精度平衡\n",
    "ResNet50：50层，更强，更慢\n",
    "ResNet101：101层，刷榜用\n",
    "ResNet152：152层，太贵，很少用\n",
    "\n",
    "数字 = 卷积层的数量\n",
    "数字越大 = 残差块越多 = 更深 = 更强但更慢\n",
    "\n",
    "实际中最常用：ResNet34 或 ResNet50\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 需要会的代码\n",
    "\n",
    "### 调用PyTorch自带的ResNet\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "# 加载预训练好的ResNet（别人已经训练好的）\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "```\n",
    "\n",
    "**不需要自己写残差块！**\n",
    "\n",
    "### 微调时改最后一层（这个要会！）\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# 第1步：加载预训练的ResNet18\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# 第2步：看看最后一层长什么样\n",
    "print(resnet.fc)    \n",
    "# 输出：Linear(in_features=512, out_features=1000)\n",
    "# 默认输出1000类（ImageNet的类别数）\n",
    "\n",
    "# 第3步：改成你需要的类别数\n",
    "resnet.fc = nn.Linear(512, 10)    # 改成10类\n",
    "# 或者\n",
    "resnet.fc = nn.Linear(512, 2)     # 改成2类（比如猫狗分类）\n",
    "```\n",
    "\n",
    "**为什么要改最后一层？**\n",
    "\n",
    "```\n",
    "预训练的ResNet是在ImageNet上训练的（1000类）\n",
    "你的任务可能只有10类或2类\n",
    "所以要把最后一层的输出改成你的类别数\n",
    "```\n",
    "\n",
    "### 微调的完整流程\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. 加载预训练模型\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# 2. 冻结所有层（不训练前面的层）\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. 改最后一层（这一层要训练）\n",
    "resnet.fc = nn.Linear(512, 你的类别数)\n",
    "# 新创建的层默认requires_grad=True，会被训练\n",
    "\n",
    "# 4. 搬到GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet.to(device)\n",
    "\n",
    "# 5. 正常训练就行了\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、总结\n",
    "\n",
    "### 概念理解\n",
    "\n",
    "```\n",
    "1. ResNet = 普通网络 + 残差连接（+x）\n",
    "2. 残差连接让信息可以跳过某些层\n",
    "3. 好处：网络再深也能训练\n",
    "4. 残差连接是现代深度学习的标配（到处都用）\n",
    "```\n",
    "\n",
    "### 代码（会这些就够了）\n",
    "\n",
    "```python\n",
    "# 加载预训练ResNet\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# 改最后一层\n",
    "resnet.fc = nn.Linear(512, 类别数)\n",
    "\n",
    "# 冻结前面的层（微调时）\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "resnet.fc = nn.Linear(512, 类别数)  # 新层自动可训练\n",
    "```\n",
    "\n",
    "### 记忆\n",
    "\n",
    "```\n",
    "ResNet = 每隔几层加一条捷径（输出 = 层的结果 + 输入）\n",
    "用的时候直接调用 models.resnet18/34/50\n",
    "改最后一层适配你的任务\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27ccc3-ee63-43ff-967c-471e3ee78fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
