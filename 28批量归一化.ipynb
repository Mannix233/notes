{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac93ed4-3d26-428e-8fb9-d642f8161159",
   "metadata": {},
   "source": [
    "# 批量归一化 Batch Normalization\n",
    "\n",
    "---\n",
    "\n",
    "## 一、为什么需要批量归一化？\n",
    "\n",
    "### 深层网络的问题\n",
    "\n",
    "```\n",
    "网络很深的时候（比如100层）\n",
    "\n",
    "反向传播：梯度从上往下传\n",
    "    顶层：梯度大 → 更新快 → 很快收敛\n",
    "    底层：梯度小 → 更新慢 → 慢慢才动\n",
    "\n",
    "问题：\n",
    "    底层一动 → 底层提取的特征变了\n",
    "    → 顶层之前学的东西全废了\n",
    "    → 顶层又得重新学\n",
    "    → 底层又动了\n",
    "    → 顶层又得重新学\n",
    "    → 恶性循环！收敛巨慢\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "盖楼：\n",
    "    地基（底层）还在慢慢调整\n",
    "    上面的楼层（顶层）已经装修好了\n",
    "    结果地基一动，上面全裂了，又得重新装修\n",
    "\n",
    "批量归一化就是：\n",
    "    让每一层的输出都保持在一个稳定的范围内\n",
    "    这样不管底层怎么变，上面的变化都不会太剧烈\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、批量归一化到底做了什么？\n",
    "\n",
    "### 核心操作\n",
    "\n",
    "```\n",
    "对每一层的输出：\n",
    "\n",
    "第1步：算这个batch里的均值和方差\n",
    "第2步：减均值，除方差（标准化成均值0、方差1）\n",
    "第3步：再乘γ，加β（学一个新的均值和方差）\n",
    "\n",
    "公式：\n",
    "    输出 = γ × (输入 - 均值) / 方差 + β\n",
    "    \n",
    "    γ和β是可学习的参数\n",
    "```\n",
    "\n",
    "### 为什么要第3步？\n",
    "\n",
    "```\n",
    "第2步已经标准化了，为什么还要乘γ加β？\n",
    "\n",
    "因为均值0方差1不一定是最好的分布\n",
    "网络自己学出一个合适的均值和方差可能更好\n",
    "\n",
    "γ学出来的就是\"最佳方差\"\n",
    "β学出来的就是\"最佳均值\"\n",
    "\n",
    "但因为γ和β变化很慢（受学习率控制）\n",
    "所以整体分布是稳定的\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "第2步 = 把所有人的身高统一成平均170cm\n",
    "第3步 = 但篮球队可能需要平均185cm\n",
    "        γ和β让网络自己决定最合适的\"身高标准\"\n",
    "        \n",
    "关键是：这个标准变化很缓慢，不会剧烈波动\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、放在哪里？\n",
    "\n",
    "### 位置：卷积/全连接 → BN → 激活函数\n",
    "\n",
    "```python\n",
    "# 正确的顺序\n",
    "nn.Conv2d(1, 6, kernel_size=5),\n",
    "nn.BatchNorm2d(6),              # ← BN在卷积后面\n",
    "nn.ReLU(),                       # ← 激活在BN后面\n",
    "\n",
    "# 不要这样\n",
    "nn.Conv2d(1, 6, kernel_size=5),\n",
    "nn.ReLU(),                       # ✗ 激活在前面\n",
    "nn.BatchNorm2d(6),               # ✗ BN在后面\n",
    "```\n",
    "\n",
    "**为什么在激活函数前面？**\n",
    "\n",
    "```\n",
    "ReLU会把负数变成0\n",
    "如果先ReLU再BN：\n",
    "    所有值都是正的 → 减均值后又变成有正有负 → 奇怪\n",
    "\n",
    "先BN再ReLU：\n",
    "    先调整好分布 → 再做非线性变换 → 合理\n",
    "```\n",
    "\n",
    "### 对卷积层 vs 全连接层\n",
    "\n",
    "```\n",
    "全连接层：对每个特征（每一列）做归一化\n",
    "卷积层：对每个通道做归一化\n",
    "\n",
    "你不需要管这个区别！\n",
    "PyTorch会自动处理\n",
    "你只需要知道用哪个：\n",
    "    卷积层后面 → nn.BatchNorm2d\n",
    "    全连接层后面 → nn.BatchNorm1d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、训练 vs 推理的区别\n",
    "\n",
    "```\n",
    "训练时：\n",
    "    用当前这个batch的均值和方差\n",
    "    同时记录一个\"全局均值\"和\"全局方差\"（滑动平均）\n",
    "\n",
    "推理时：\n",
    "    不用当前batch的均值方差\n",
    "    用训练时记录的\"全局均值\"和\"全局方差\"\n",
    "\n",
    "为什么？\n",
    "    推理时可能只有1个样本\n",
    "    1个样本算不出有意义的均值和方差\n",
    "    所以用训练时统计好的全局值\n",
    "```\n",
    "\n",
    "**你不需要手动处理这个！**\n",
    "\n",
    "```python\n",
    "# 训练时\n",
    "net.train()     # 切换到训练模式 → BN用batch的均值方差\n",
    "\n",
    "# 推理时\n",
    "net.eval()      # 切换到评估模式 → BN用全局的均值方差\n",
    "```\n",
    "\n",
    "**这就是为什么你看到训练代码里总有 net.train() 和 net.eval()！**\n",
    "\n",
    "---\n",
    "\n",
    "## 五、代码：简洁实现（你需要的）\n",
    "\n",
    "### BatchNorm的使用\n",
    "\n",
    "```python\n",
    "# 卷积层后面用 BatchNorm2d，参数 = 通道数\n",
    "nn.BatchNorm2d(通道数)\n",
    "\n",
    "# 全连接层后面用 BatchNorm1d，参数 = 特征数\n",
    "nn.BatchNorm1d(特征数)\n",
    "```\n",
    "\n",
    "### LeNet + BatchNorm\n",
    "\n",
    "```python\n",
    "net = nn.Sequential(\n",
    "    # === 第1组 ===\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.BatchNorm2d(6),              # BN！参数=输出通道数6\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # === 第2组 ===\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.BatchNorm2d(16),             # BN！参数=输出通道数16\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # === 全连接 ===\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.BatchNorm1d(120),            # BN！参数=输出特征数120\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.BatchNorm1d(84),             # BN！参数=输出特征数84\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10),              # 最后输出层不加BN\n",
    ")\n",
    "```\n",
    "\n",
    "### 记忆规则\n",
    "\n",
    "```\n",
    "Conv2d(in, out, ...) → BatchNorm2d(out)\n",
    "                                    ^^^\n",
    "                                    参数就是前面Conv的输出通道数\n",
    "\n",
    "Linear(in, out)      → BatchNorm1d(out)\n",
    "                                    ^^^\n",
    "                                    参数就是前面Linear的输出大小\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、BatchNorm的效果\n",
    "\n",
    "```\n",
    "不用BN：\n",
    "    学习率只能用0.01这种小值\n",
    "    训练慢，收敛需要很多epoch\n",
    "\n",
    "用了BN：\n",
    "    学习率可以用0.1甚至更大\n",
    "    训练快，收敛更快\n",
    "    \n",
    "但最终精度差不多！\n",
    "BN主要是加速训练，不是提升精度\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、BN和Dropout的关系\n",
    "\n",
    "```\n",
    "BN的一个副作用：有轻微的正则化效果\n",
    "（因为每个batch的均值方差是随机的，相当于加了噪声）\n",
    "\n",
    "所以：\n",
    "    用了BN之后，Dropout的效果会减弱\n",
    "    很多时候用了BN就不用Dropout了\n",
    "    \n",
    "实际中：\n",
    "    ResNet等现代网络 → 用BN，不用Dropout\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 八、总结\n",
    "\n",
    "### 你需要记住的\n",
    "\n",
    "```\n",
    "1. BN做什么：减均值除方差，再乘γ加β\n",
    "2. 放在哪里：卷积/全连接后面，激活函数前面\n",
    "3. 效果：加速训练，允许更大学习率\n",
    "4. 不改变最终精度，只是训练更快\n",
    "```\n",
    "\n",
    "### 代码模板（背这个）\n",
    "\n",
    "```python\n",
    "# 卷积层后面\n",
    "nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "nn.BatchNorm2d(out_ch),     # 参数 = 输出通道数\n",
    "nn.ReLU(),\n",
    "\n",
    "# 全连接层后面\n",
    "nn.Linear(in_features, out_features),\n",
    "nn.BatchNorm1d(out_features),  # 参数 = 输出特征数\n",
    "nn.ReLU(),\n",
    "```\n",
    "\n",
    "### 训练时别忘了\n",
    "\n",
    "```python\n",
    "# 训练\n",
    "net.train()     # BN用batch统计量\n",
    "\n",
    "# 评估\n",
    "net.eval()      # BN用全局统计量\n",
    "```\n",
    "\n",
    "### BatchNorm2d vs BatchNorm1d\n",
    "\n",
    "| | BatchNorm2d | BatchNorm1d |\n",
    "|:---|:---|:---|\n",
    "| 用在哪后面 | Conv2d | Linear |\n",
    "| 参数 | 输出通道数 | 输出特征数 |\n",
    "| 数据维度 | 4维 | 2维 |\n",
    "\n",
    "**一句话：Conv后面用2d，Linear后面用1d，参数都是前一层的输出大小。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6aead-b261-4b39-b8eb-3c1d33fe3834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
