{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac93ed4-3d26-428e-8fb9-d642f8161159",
   "metadata": {},
   "source": [
    "# 批量归一化 Batch Normalization\n",
    "\n",
    "---\n",
    "\n",
    "## 一、为什么需要批量归一化？\n",
    "\n",
    "### 深层网络的问题\n",
    "\n",
    "```\n",
    "网络很深的时候（比如100层）\n",
    "\n",
    "反向传播：梯度从上往下传\n",
    "    顶层：梯度大 → 更新快 → 很快收敛\n",
    "    底层：梯度小 → 更新慢 → 慢慢才动\n",
    "\n",
    "问题：\n",
    "    底层一动 → 底层提取的特征变了\n",
    "    → 顶层之前学的东西全废了\n",
    "    → 顶层又得重新学\n",
    "    → 底层又动了\n",
    "    → 顶层又得重新学\n",
    "    → 恶性循环！收敛巨慢\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "盖楼：\n",
    "    地基（底层）还在慢慢调整\n",
    "    上面的楼层（顶层）已经装修好了\n",
    "    结果地基一动，上面全裂了，又得重新装修\n",
    "\n",
    "批量归一化就是：\n",
    "    让每一层的输出都保持在一个稳定的范围内\n",
    "    这样不管底层怎么变，上面的变化都不会太剧烈\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 二、批量归一化到底做了什么？\n",
    "\n",
    "### 核心操作\n",
    "\n",
    "```\n",
    "对每一层的输出：\n",
    "\n",
    "第1步：算这个batch里的均值和方差\n",
    "第2步：减均值，除方差（标准化成均值0、方差1）\n",
    "第3步：再乘γ，加β（学一个新的均值和方差）\n",
    "\n",
    "公式：\n",
    "    输出 = γ × (输入 - 均值) / 方差 + β\n",
    "    \n",
    "    γ和β是可学习的参数\n",
    "```\n",
    "\n",
    "### 为什么要第3步？\n",
    "\n",
    "```\n",
    "第2步已经标准化了，为什么还要乘γ加β？\n",
    "\n",
    "因为均值0方差1不一定是最好的分布\n",
    "网络自己学出一个合适的均值和方差可能更好\n",
    "\n",
    "γ学出来的就是\"最佳方差\"\n",
    "β学出来的就是\"最佳均值\"\n",
    "\n",
    "但因为γ和β变化很慢（受学习率控制）\n",
    "所以整体分布是稳定的\n",
    "```\n",
    "\n",
    "**比喻：**\n",
    "\n",
    "```\n",
    "第2步 = 把所有人的身高统一成平均170cm\n",
    "第3步 = 但篮球队可能需要平均185cm\n",
    "        γ和β让网络自己决定最合适的\"身高标准\"\n",
    "        \n",
    "关键是：这个标准变化很缓慢，不会剧烈波动\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、放在哪个位置？\n",
    "\n",
    "### 位置：卷积/全连接 → BN → 激活函数\n",
    "\n",
    "```python\n",
    "# 正确的顺序\n",
    "nn.Conv2d(1, 6, kernel_size=5),\n",
    "nn.BatchNorm2d(6),              # ← BN在卷积后面\n",
    "nn.ReLU(),                       # ← 激活在BN后面\n",
    "\n",
    "# 不要这样\n",
    "nn.Conv2d(1, 6, kernel_size=5),\n",
    "nn.ReLU(),                      \n",
    "nn.BatchNorm2d(6),   # ✗ BN在后面错了！\n",
    "```\n",
    "\n",
    "**为什么在激活函数前面？**\n",
    "\n",
    "```\n",
    "ReLU会把负数变成0\n",
    "如果先ReLU再BN：\n",
    "    所有值都是正的 → 减均值后又变成有正有负 → 奇怪又回去了\n",
    "\n",
    "先BN再ReLU：\n",
    "    先调整好分布 → 再做非线性变换 → 合理\n",
    "```\n",
    "\n",
    "### 对卷积层 vs 全连接层\n",
    "\n",
    "```\n",
    "全连接层：对每个特征（每一列）做归一化\n",
    "卷积层：对每个通道做归一化\n",
    "\n",
    "但实际不需要管这个区别！\n",
    "PyTorch会自动处理\n",
    "你只需要知道用哪个：\n",
    "    卷积层后面 → nn.BatchNorm2d\n",
    "    全连接层后面 → nn.BatchNorm1d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、训练 vs 推理的代码区别\n",
    "\n",
    "# 先搞懂 net.train() 和 net.eval()\n",
    "\n",
    "**训练 = 平时上课**\n",
    "\n",
    "```\n",
    "老师每节课都考一次小测验\n",
    "每次小测验的平均分都不一样：\n",
    "    第1次小测：班级平均75分\n",
    "    第2次小测：班级平均82分\n",
    "    第3次小测：班级平均68分\n",
    "    ...\n",
    "\n",
    "老师一边上课，一边偷偷记录：\n",
    "    \"这学期到目前为止，总体平均大概是76分\"\n",
    "    \n",
    "这个\"总体平均\"每次小测后都更新一下\n",
    "越来越准\n",
    "```\n",
    "\n",
    "**推理 = 期末考试**\n",
    "\n",
    "```\n",
    "期末考试只有1个同学来补考\n",
    "就他1个人，你怎么算\"班级平均分\"？\n",
    "\n",
    "算不了！1个人没法代表班级水平\n",
    "\n",
    "怎么办？\n",
    "    用这学期记录的\"总体平均76分\"就好了！\n",
    "    这个数是靠平时很多次小测积累出来的\n",
    "    已经很准了\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 翻译成神经网络\n",
    "\n",
    "```\n",
    "平时上课（训练）：\n",
    "    每次来一个batch（比如256张图片）\n",
    "    用这256张图片算均值和方差 ← 就像每次小测验的平均分\n",
    "    同时更新\"总体均值\"和\"总体方差\" ← 就像记录学期总平均\n",
    "\n",
    "期末考试（推理/预测）：\n",
    "    可能只来1张图片\n",
    "    1张图片算均值方差没意义\n",
    "    直接用训练时积累的\"总体均值\"和\"总体方差\" ← 用学期总平均\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## net.train() 和 net.eval() 在干什么？\n",
    "\n",
    "```python\n",
    "net.train()\n",
    "```\n",
    "\n",
    "```\n",
    "告诉网络：\"现在是平时上课\"\n",
    "→ BN用当前这批学生（batch）的均值方差\n",
    "→ 同时更新总体均值方差\n",
    "```\n",
    "\n",
    "```python\n",
    "net.eval()\n",
    "```\n",
    "\n",
    "```\n",
    "告诉网络：\"现在是期末考试\"\n",
    "→ BN用积累好的总体均值方差\n",
    "→ 不再更新任何东西\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 为什么不能一直用batch的均值方差？\n",
    "\n",
    "```\n",
    "训练时：每次来256张图片，算均值方差 → 没问题，256张够多\n",
    "\n",
    "推理时：可能就来1张图片\n",
    "    1张图片的\"均值\" = 它自己\n",
    "    1张图片的\"方差\" = 0\n",
    "    \n",
    "    你拿这个去做归一化？\n",
    "    (x - x) / 0 = ???  → 直接炸了！\n",
    "    \n",
    "    所以推理时必须用一个\"靠谱的\"均值方差\n",
    "    训练时积累的总体值就是那个\"靠谱的\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 你只需要记住的\n",
    "\n",
    "```python\n",
    "# 训练的时候，开头写这个\n",
    "net.train()\n",
    "\n",
    "# 要预测/评估的时候，写这个\n",
    "net.eval()\n",
    "\n",
    "# 就这两行！PyTorch自动处理所有细节\n",
    "# 你不需要知道里面怎么算的\n",
    "```\n",
    "\n",
    "### 典型的训练代码\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()                      # ← 上课模式\n",
    "    for X, y in train_loader:\n",
    "        # 训练代码...\n",
    "    \n",
    "    net.eval()                       # ← 考试模式\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # 评估代码...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 一句话总结\n",
    "\n",
    "```\n",
    "net.train() = 告诉BN\"用当前batch的统计量\"\n",
    "net.eval()  = 告诉BN\"用训练时积累的统计量\"\n",
    "\n",
    "为什么？因为推理时可能只有1张图片，算不了均值方差\n",
    "所以用训练时攒下来的\n",
    "\n",
    "你只需要：训练前写train()，评估前写eval()\n",
    "其他都不用管\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 五、代码实现\n",
    "\n",
    "### BatchNorm的使用\n",
    "\n",
    "```python\n",
    "# 卷积层后面用 BatchNorm2d，参数 = 通道数\n",
    "nn.BatchNorm2d(通道数)\n",
    "\n",
    "# 全连接层后面用 BatchNorm1d，参数 = 特征数\n",
    "nn.BatchNorm1d(特征数)\n",
    "```\n",
    "\n",
    "### LeNet + BatchNorm\n",
    "\n",
    "```python\n",
    "net = nn.Sequential(\n",
    "    # === 第1组 ===\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.BatchNorm2d(6),              # BN！参数=输出通道数6\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # === 第2组 ===\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.BatchNorm2d(16),             # BN！参数=输出通道数16\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # === 全连接 ===\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.BatchNorm1d(120),            # BN！参数=输出特征数120\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.BatchNorm1d(84),             # BN！参数=输出特征数84\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10),              # 最后输出层不加BN\n",
    ")\n",
    "```\n",
    "\n",
    "### 记忆规则\n",
    "\n",
    "```\n",
    "Conv2d(in, out, ...) → BatchNorm2d(out)\n",
    "                                    ^^^\n",
    "     参数就是前面Conv的输出通道数\n",
    "\n",
    "Linear(in, out)      → BatchNorm1d(out)\n",
    "                                    ^^^\n",
    "     参数就是前面Linear的输出大小\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、BatchNorm的效果\n",
    "\n",
    "```\n",
    "不用BN：\n",
    "    学习率只能用0.01这种小值\n",
    "    训练慢，收敛需要很多epoch\n",
    "\n",
    "用了BN：\n",
    "    学习率可以用0.1甚至更大\n",
    "    训练快，收敛更快\n",
    "    \n",
    "但最终精度差不多！\n",
    "BN主要是加速训练，不是提升精度\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、BN和Dropout的关系\n",
    "\n",
    "```\n",
    "BN的一个副作用：有轻微的正则化效果\n",
    "（因为每个batch的均值方差是随机的，相当于加了噪声）\n",
    "\n",
    "所以：\n",
    "    用了BN之后，Dropout的效果会减弱\n",
    "    很多时候用了BN就不用Dropout了\n",
    "    \n",
    "实际中：\n",
    "    ResNet等现代网络 → 用BN，不用Dropout\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 八、总结\n",
    "\n",
    "### 需要记住的\n",
    "\n",
    "```\n",
    "1. BN做什么：减均值除方差，再乘γ加β\n",
    "2. 放在哪里：卷积/全连接后面，激活函数前面\n",
    "3. 效果：加速训练，允许更大学习率\n",
    "4. 不改变最终精度，只是训练更快\n",
    "```\n",
    "\n",
    "### 代码模板\n",
    "\n",
    "```python\n",
    "# 卷积层后面\n",
    "nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "nn.BatchNorm2d(out_ch),     # 参数 = 输出通道数\n",
    "nn.ReLU(),\n",
    "\n",
    "# 全连接层后面\n",
    "nn.Linear(in_features, out_features),\n",
    "nn.BatchNorm1d(out_features),  # 参数 = 输出特征数\n",
    "nn.ReLU(),\n",
    "```\n",
    "\n",
    "### 训练时别忘了\n",
    "\n",
    "```python\n",
    "# 训练\n",
    "net.train()     # BN用batch统计量\n",
    "\n",
    "# 评估\n",
    "net.eval()      # BN用全局统计量\n",
    "```\n",
    "\n",
    "### BatchNorm2d vs BatchNorm1d\n",
    "\n",
    "| | BatchNorm2d | BatchNorm1d |\n",
    "|:---|:---|:---|\n",
    "| 用在哪后面 | Conv2d | Linear |\n",
    "| 参数 | 输出通道数 | 输出特征数 |\n",
    "| 数据维度 | 4维 | 2维 |\n",
    "\n",
    "**一句话：Conv后面用2d，Linear后面用1d，参数都是前一层的输出大小。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6aead-b261-4b39-b8eb-3c1d33fe3834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
